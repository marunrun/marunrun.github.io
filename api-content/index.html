{"posts":[{"title":"k8s安装kafka  kafka-ui ","content":"k8s安装kafka 并开放外部访问 参考文档： kafka 19.1.3 · bitnami/bitnami (artifacthub.io) charts/values.yaml at main · bitnami/charts (github.com) 添加 helm repo helm bitnami add bitnami https://charts.bitnami.com/bitnami 如果使用PV 请提前配置好storageclass 使用以下命令查看 kubectl get storageclass 建议使用values.yaml进行配置 kafka 默认配置参考：charts/values.yaml at main · bitnami/charts (github.com) 对应的zookeeper默认配置 charts/bitnami/zookeeper at main · bitnami/charts (github.com) 如果使用PV 需要配置权限,不然会出现没有权限的情况 volumePermissions: enabled: true zookeeper: volumePermissions: enabled: true 如需使用外网访问请参考文档charts/bitnami/kafka at main · bitnami/charts (github.com)添加配置 我这里使用nodePort方式： externalAccess: enabled: true service: type: &quot;NodePort&quot; autoDiscovery: enabled: true serviceAccount: create: true rbac: create: true 注意这里需要创建额外的svc 以我这里为例，三个cluster 需要创建三个svc，kafka-0-external kafka-1-external kafka-2-external kind: Service apiVersion: v1 metadata: name: kafka-{num}-external namespace: database spec: ports: - name: port protocol: TCP port: 9094 targetPort: 9094 selector: statefulset.kubernetes.io/pod-name: kafka-{num} type: NodePort sessionAffinity: None externalTrafficPolicy: Cluster 这样运行起来，就可以通过nodeip:nodeport 来访问k8s内的kafka服务了 $&gt; kubectl get svc -n database kafka-0-external NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kafka-0-external NodePort 10.103.255.3 &lt;none&gt; 9094:32402/TCP 19h 以上面的这个为例，我们的节点ip是：192.168.1.3 那么在k8s 外部，就可以使用192.168.1.3:32402 以此类推，其他两个cluster也可以这样访问，不过是nodeport不一样而已 ❯ kubectl get svc -n database NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kafka-0-external NodePort 10.103.255.3 &lt;none&gt; 9094:32402/TCP 19h kafka-1-external NodePort 10.106.84.112 &lt;none&gt; 9094:31626/TCP 19h kafka-2-external NodePort 10.100.211.212 &lt;none&gt; 9094:32458/TCP 19h 还可以通过统一的NodePort-svc 将headless暴露至外部 参考：将 headless service 映射到外网 - 简书 (jianshu.com) ❯ kubectl expose svc kafka-headless --name=kafka-external --type=LoadBalancer --port=9094 --target-port=9094 -n database ❯ kubectl get svc -n database kafka-external NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kafka-external LoadBalancer 10.105.60.175 192.168.1.244 9094:32002/TCP 19h 这样也可以使用192.168.1.3:32002 搭建kafka-ui 文档：provectus/kafka-ui: Open-Source Web UI for Apache Kafka Management (github.com) 先修改一下配置文件，k8s内部的kafka 可以直接使用headless服务地址 kafka-ui-config.yaml: apiVersion: v1 kind: ConfigMap metadata: name: kafka-ui-config namespace: database data: config.yml: |- kafka: clusters: - name: k8s bootstrapServers: kafka-headless.database.svc.cluster.local:9092 auth: type: disabled management: health: ldap: enabled: false 使用k8s搭建 kubectl apply -f kafka-ui-config.yaml helm repo add kafka-ui https://provectus.github.io/kafka-ui helm install my-kafka-ui -n database --set yamlApplicationConfigConfigMap.name=&quot;kafka-ui-config&quot;,yamlApplicationConfigConfigMap.keyName=&quot;config.yml&quot; kafka-ui/kafka-ui --version 0.4.5 ","link":"https://blog.marun.run/k8s-an-zhuang-kafka-kafka-ui/"},{"title":"Istio 服务网格启动顺序","content":"记录一下istio服务网格的问题 前言 如果启用了istio 服务网格，那么会自动的注入 istio-proxy sidecar 这里的proxy容器与我们的业务容器启动顺序是不确定的，如果在istio-proxy未正确启动之前，在业务容器中就尝试对外进行网络通信，这个时候就会出现问题。 例如：数据库连接 redis连接初始化问题 所以需要注意在业务代码中做好重试，当然现在大多数框架都自带重连机制。 但在实际业务场景下，一旦开启了日志告警，每次服务重启就会导致一堆告警，会很麻烦，这个时候就还是需要控制一下istio 服务网格容器的启动。 解决方法 高版本使用istio自带配置 自Istio 1.7 版本开始，增加了holdApplicationUntilProxyStarts配置项，解决上述问题 参考官网的changelog 低版本使用手动控制 istio-proxy 有一个健康检查的接口，我们可以通过主动探测的方式来确认istio-proxy启动完成，最后再启动自己的服务 while [[ &quot;$(curl -s -o /dev/null -w ''%{http_code}'' localhost:15020/healthz/ready)&quot; != '200' ]]; do echo Waiting for Sidecar;sleep 1; done; echo Sidecar available; ./http 注意以上脚本 最后的./http 请替换成自己的业务运行脚本 如果运行参数过多，可以自行编写start.sh 启动脚本，或者使用supervision 参考： https://zhuanlan.zhihu.com/p/369301902 ","link":"https://blog.marun.run/istio-fu-wu-wang-ge-qi-dong-shun-xu/"},{"title":"kotlin jackson  missing type id property '@class'","content":"使用kotlin 搭配redis 存储json格式数据的问题 Redis 配置如下 @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate&lt;String, Object&gt; redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(redisConnectionFactory); // json 序列化 redisTemplate.setValueSerializer(RedisSerializer.json()); RedisSerializer&lt;String&gt; stringRedisSerializer = RedisSerializer.string(); // 使用StringRedisSerializer来序列化和反序列化redis的key值 redisTemplate.setKeySerializer(stringRedisSerializer); // hash的key也采用String的序列化方式 redisTemplate.setHashKeySerializer(stringRedisSerializer); // hash的value序列化方式采用jackson redisTemplate.setHashValueSerializer(RedisSerializer.json()); return redisTemplate; } 在使用redistemplate 时，我们使用json格式默认存储value，然后发现每次存进去的数据都会少@class 在取出数据反序列化的时候会报错 GenericJackson2JsonRedisSerializer 文件 public GenericJackson2JsonRedisSerializer(@Nullable String classPropertyTypeName) { this(new ObjectMapper()); registerNullValueSerializer(this.mapper, classPropertyTypeName); if (StringUtils.hasText(classPropertyTypeName)) { this.mapper.enableDefaultTypingAsProperty(DefaultTyping.NON_FINAL, classPropertyTypeName); } else { this.mapper.enableDefaultTyping(DefaultTyping.NON_FINAL, As.PROPERTY); } } 查看源码，发现在这个jackson的配置中，默认使用了DefaultTyping.NON_FINAL 而在 kotlin中 我们的类 默认是final的 需要添加在类前面添加open修饰符 或者你也可以使用使用自定义objectMapper 并设置 ObjectMapper.DefaultTyping.EVERYTHING ","link":"https://blog.marun.run/kotlin-jackson-missing-type-id-property-class/"},{"title":"Mybastis Plus 使用mapper.xml","content":"记录一下mybatis plus 中mapper的使用 mybatis plus官网 分页器配置 @Configuration public class MybatisPlusConfig { @Bean public MybatisPlusInterceptor mybatisPlusInterceptor() { MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor(); interceptor.addInnerInterceptor(new PaginationInnerInterceptor(DbType.MYSQL)); return interceptor; } } mapper.xml: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;mapper.HiProcinstMapper&quot;&gt; &lt;select id=&quot;getTodoList&quot; resultType=&quot;model.HiProcinstModel&quot;&gt; select PROC.* from ACT_HI_PROCINST PROC left join ACT_RU_TASK TASK on PROC.PROC_INST_ID_ = TASK.PROC_INST_ID_ ${ew.customSqlSegment} &lt;/select&gt; &lt;!-- --&gt; &lt;select id=&quot;getDoneList&quot; resultType=&quot;model.HiProcinstModel&quot;&gt; select PROC.* from ACT_HI_PROCINST PROC left join ACT_HI_TASKINST TASK on PROC.PROC_INST_ID_ = TASK.PROC_INST_ID_ ${ew.customSqlSegment} &lt;/select&gt; &lt;/mapper&gt; mapper.kt package mapper; import com.baomidou.mybatisplus.core.conditions.Wrapper import model.HiProcinstModel; import com.baomidou.mybatisplus.core.mapper.BaseMapper; import com.baomidou.mybatisplus.core.metadata.IPage import com.baomidou.mybatisplus.core.toolkit.Constants import org.apache.ibatis.annotations.Param import org.apache.ibatis.annotations.Select interface HiProcinstMapper : BaseMapper&lt;HiProcinstModel&gt; { /** * 获取代办列表 */ fun getTodoList( page:IPage&lt;HiProcinstModel&gt;,@Param(Constants.WRAPPER) wrapper: Wrapper&lt;HiProcinstModel&gt;) : IPage&lt;HiProcinstModel&gt; /** * 获取已处理列表 */ fun getDoneList( page:IPage&lt;HiProcinstModel&gt;,@Param(Constants.WRAPPER) wrapper: Wrapper&lt;HiProcinstModel&gt;) : IPage&lt;HiProcinstModel&gt; } 我们只在mapper.xml中写了关联查询的语句，其他查询条件和分页我们还是使用了mybatisPlus的分页插件 和QueryWrapper 在Service中调用 @Service open class HiProcinstServiceImpl : ServiceImpl&lt;HiProcinstMapper, HiProcinstModel&gt;(), IHiProcinstService { @Resource lateinit var processEngine: ProcessEngine /** * 待审批列表 */ override fun getTodoList(userId: String, pageParams: PageParams): Pagination&lt;ProcInstVo&gt; { val queryWrapper = QueryWrapper&lt;HiProcinstModel&gt;() // 任务分配给当前用户 .eq(&quot;TASK.${HiTaskinstModel.ASSIGNEE_}&quot;, userId) // 并且是活动中的任务 .eq(&quot;PROC.${HiProcinstModel.STATE_}&quot;, HistoricProcessInstance.STATE_ACTIVE) .orderByAsc(&quot;PROC.${HiProcinstModel.START_TIME_}&quot;) val todoList = this.baseMapper.getTodoList(Page(pageParams.page, pageParams.pageSize), queryWrapper) return Pagination(this.convert2Vo(todoList)) } /** * 已处理流程实例 */ override fun getDoneList(userId: String, pageParams: PageParams): Pagination&lt;ProcInstVo&gt; { val queryWrapper = QueryWrapper&lt;HiProcinstModel&gt;() // 任务分配给当前用户 .eq(&quot;TASK.${HiTaskinstModel.ASSIGNEE_}&quot;, userId) // 并且任务已完成 .isNotNull(&quot;TASK.${HiTaskinstModel.END_TIME_}&quot;) .orderByDesc(&quot;PROC.${HiProcinstModel.START_TIME_}&quot;) val doneList = this.baseMapper.getDoneList(Page(pageParams.page, pageParams.pageSize), queryWrapper) return Pagination(this.convert2Vo(doneList)) } } ","link":"https://blog.marun.run/mybastis-plus-shi-yong-mapperxml/"},{"title":"Springboot 注册ServletContextListener  与context-params使用","content":"背景说明： 在springboot中使用 camunda-rest 会有配置日期格式的需求 而按照文档来配置 需要配置在WEB-INF/web.xml里，在springboot中我们并不需要配置这个web.xml 注册camunda的Listenter到springboot中： @Configuration public class MyConfiguration { /** * 把自定义的listener注册到容器中 * @return */ @Bean public ServletListenerRegistrationBean customJacksonDateFormatListener(){ ServletListenerRegistrationBean&lt;CustomJacksonDateFormatListener&gt; registrationBean = new ServletListenerRegistrationBean&lt;&gt;(new CustomJacksonDateFormatListener()); return registrationBean; } } 在application.yaml中配置context-param server: servlet: context-parameters: org.camunda.bpm.engine.rest.jackson.dateFormat: yyyy-MM-dd HH:mm:ss ","link":"https://blog.marun.run/springboot-zhu-ce-servletcontextlistener-yu-context-params-shi-yong/"},{"title":"Mac m1 安装swoole  碰到的问题","content":"使用pecl install swoole 报错 一.openssl not found 使用brew install openssl --enable-openssl --with-openssl-dir=/opt/homebrew/Cellar/openssl@3/3.0.1/ 二. library not found for -latomic 使用 brew install gcc 切换gcc为brew安装的版本 export CC=gcc-11 export CXX=g++-11 三. pcre2.h not found 使用brew install pcre2 然后软链到你当前使用的php版本下的pcre ln -s /opt/homebrew/Cellar/pcre2/10.39/include/pcre2.h /opt/homebrew/Cellar/php@7.3/7.3.31_1/include/php/ext/pcre/pcre2.h 以上路径需要改成您自己的路径 ","link":"https://blog.marun.run/mac-m1-an-zhuang-swoole-peng-dao-de-wen-ti/"},{"title":"activiti, camunda, 会签 或签","content":"会签，或签，依次审批借助于multi-instance Multi-Instance | Camunda Cloud Docs Activiti User Guide 多实例任务： 可以理解为可重复执行的任务 如何定义重复执行的次数 ？ 一. loopCardinality 可以设置一个固定的循环上限。 &lt;multiInstanceLoopCharacteristics isSequential=&quot;true&quot;&gt; &lt;loopCardinality&gt;5&lt;/loopCardinality&gt; &lt;/multiInstanceLoopCharacteristics&gt; 当然你也可以在这里写一个表达式，用于计算出一个正整数循环次数 &lt;multiInstanceLoopCharacteristics isSequential=&quot;true&quot;&gt; &lt;loopCardinality&gt;${nrOfOrders-nrOfCancellations}&lt;/loopCardinality&gt; &lt;/multiInstanceLoopCharacteristics&gt; 二. Collection 设置一个循环访问集合 设置固定的可循环访问的集合：collection=[&quot;marin&quot;, &quot;monkey&quot;] &lt;userTask id=&quot;miTasks&quot; name=&quot;My Task&quot; activiti:assignee=&quot;${assignee}&quot;&gt; &lt;multiInstanceLoopCharacteristics isSequential=&quot;false&quot; activiti:collection=&quot;[&quot;marin&quot;,&quot;monkey&quot;]&quot; activiti:elementVariable=&quot;assigner&quot; &gt; &lt;/multiInstanceLoopCharacteristics&gt; &lt;/userTask&gt; 并通过elementVariable将循环取出的值赋值给一个变量。 在上面的这个例子中，我们设置一个固定的集合[&quot;marin&quot;,&quot;monkey&quot;],将elementVariable 设置为assigner。并将userTask的assignee设置为${assigner} 。 这也就是代表着，在这个多例任务中，会创建两个userTask,并依次将这两个任务分配给marin,monkey。 使用变量填充 collection。 &lt;userTask id=&quot;miTasks&quot; name=&quot;My Task&quot; activiti:assignee=&quot;${assigner}&quot;&gt; &lt;multiInstanceLoopCharacteristics isSequential=&quot;true&quot; activiti:collection=&quot;assignerList&quot; activiti:elementVariable=&quot;assigner&quot; &gt; &lt;/multiInstanceLoopCharacteristics&gt; &lt;/userTask&gt; 我们可以在发起实例的时候传入一个变量assignerList RuntimeService runtimeService = processEngine.getRuntimeService(); Map&lt;String, Object&gt; variables = new HashMap&lt;&gt;(); variables.put(&quot;assignerList&quot;, Arrays.asList(&quot;marin&quot;, &quot;monkey&quot;, &quot;roy&quot;)); runtimeService.startProcessInstanceByKey(&quot;multiple&quot;, variables); 使用spring，服务 &lt;userTask id=&quot;miTasks&quot; name=&quot;My Task&quot; activiti:assignee=&quot;${assigner}&quot;&gt; &lt;multiInstanceLoopCharacteristics isSequential=&quot;false&quot; activiti:collection=&quot;${userService.getApprovalAssigners()}&quot; activiti:elementVariable=&quot;assigner&quot; &gt; &lt;/multiInstanceLoopCharacteristics&gt; &lt;/userTask&gt; 如果我们使用spring boot，我们可以在这里调用代码中的某个服务某个方法来获取一个集合 注意⚠️，这里的userService 必须在spring 中注册为 Spring Bean Service 注意⚠️，一个多实例活动必须要有loopCardinality，或者collection 其中一个，否则执行出错。 如何终止运行？ 默认在所有多实例任务都完成之后，那么这个多实例任务也就完成了。 或者你可以定一个完成条件：completionCondition &lt;userTask id=&quot;miTasks&quot; name=&quot;My Task&quot; activiti:assignee=&quot;${assigner}&quot;&gt; &lt;multiInstanceLoopCharacteristics isSequential=&quot;false&quot; activiti:collection=&quot;${userService.getApprovalAssigners()}&quot; activiti:elementVariable=&quot;assigner&quot; &gt; &lt;completionCondition&gt;${nrOfCompletedInstances/nrOfInstances &gt;= 0.6 }&lt;/completionCondition&gt; &lt;/multiInstanceLoopCharacteristics&gt; &lt;/userTask&gt; 以下为默认的多实例变量。 nrOfInstances: 实例总数（任务总数），对应collection的size，loopCardinality的值 nrOfActiveInstances: 当前活动的实例数，对于串行的多实例来说，这个值一直都是1 nrOfCompletedInstances: 已完成实例数 上面的例子代表，如果有60%的任务已经完成，那么当前多实例就完成了。 当然，这里也可以写表达式，或者调用spring 的service，只要返回的是一个bool值就ok 并签： 并签对应 parallel multi-instance 并行的多实例，如下图（图片来自canmunda） 例如：这一次多实例审批，指定了两个人。 会同时生成两个task，这两个task是相互独立的，只有当这两个独立的task都完成时才会走到下一步。 当然我们可以在多实例任务上定义终止条件，提前结束多例任务。 后文的或签就是基于此实现。 并行的多实例是在流程运行至此的时候，一次生成多个任务。 依次审批： 依次审批对应 sequential multi-instance 串行的多实例 首先生成第一个任务，只有第一个任务完成后才会生成第二个任务，直到所有的任务完成 或签： 或签对应的也是parallel multi-instance, 不同的是，我们会设置一个completionCondition=&quot;${nrOfCompletedInstances==1}&quot; 也就是我们会一次生成多个任务，只要有其中一个任务完成，那么整个多实例任务都被视为完成了。 ","link":"https://blog.marun.run/activiti-camunda-hui-qian-huo-qian/"},{"title":"mysql view 字符集排序报错","content":"General error: 1267 Illegal mix of collations 在使用mysql视图时，报错 SQLSTATE[HY000]: General error: 1267 Illegal mix of collations (utf8mb4_general_ci,COERCIBLE) and (utf8mb4_unicode_ci,COERCIBLE) for operation '=' 通过命令 SHOW CREATE TABLE view_table_name; 查看字符集，以及排序规则 character_set_client collation_connection utf8mb4 utf8mb4_general_ci 发现视图的排序规则错误。 删除原有view 并重建 -- 设置排序规则 SET collation_connection = utf8mb4_unicode_ci; -- 删除原有的view drop VIEW view_table_name; -- 重新新建view create view view_table_name .... 如果是字符集报错，同理修改即可。 SET character_set_client = utf8mb4; SET character_set_results = utf8mb4; SET character_set_connection = utf8mb4; 参考资料： https://stackoverflow.com/questions/9422189/why-is-my-view-utf8-and-how-can-i-change-it-to-latin1 ","link":"https://blog.marun.run/mysql-view-zi-fu-ji-pai-xu-bao-cuo/"},{"title":"部署 packagist私服 搭配gitlab 私服使用 ","content":"部署packagist 搭配gitlab使用 源码地址：composer/packagist 笔者在写这篇文章的时候对应的commit sha :aa0d63fd6ebbfd991aa3ab652d33c6c8e463543a 因为packagist没有版本的概念 所以使用commit sha 的方式来记录一下 环境安装 由于最新版使用的是php8.0,所以来安装一下8.0 使用apt安装php8.0 sudo apt install php8.0 php8.0-bcmath php8.0-redis php8.0-cli php8.0-zip php8.0-mysql php8.0-http php8.0-curl php8.0-common php8.0-raphf php8.0-dev php8.0-mbstring php8.0-dom php8.0-xml 安装过程比较简单，基本如上 在安装完成后，composer install 时出现unrecognised compile-time option bit(s) 按照这个issure 只用执行以下命令即可（如果你没有出现任何问题，无视即可） sudo apt-get install --only-upgrade libpcre2-16-0 libpcre2-32-0 libpcre2-8-0 libpcre2-dev libpcre2-posix2 项目部署 git clone https://github.com/composer/packagist.git &amp;&amp; cd packagist 项目配置 拉取代码之后，先配置一下redis,database 等等 打开.env文件 # 数据库配置， 修改成自己的配置，并确保拥有packagist库 DATABASE_URL=&quot;mysql://root:123456@127.0.0.1:3306/packagist?serverVersion=8.0&quot; # ALGOLIA 配置,搜索用。 可前往 https://www.algolia.com/ 注册，免费使用，如果只是尝试本地部署，可忽略 ALGOLIA_APP_ID=xxx ALGOLIA_ADMIN_KEY=xxx ALGOLIA_SEARCH_KEY=xxx ALGOLIA_INDEX_NAME=xxx # redis 配置，可换成自己 REDIS_URL=redis://localhost # 谷歌的一个验证，我在使用时，这里会有点问题，下面会讲。 ###&gt; beelab/recaptcha2-bundle ### APP_RECAPTCHA_ENABLED=false APP_RECAPTCHA_SITE_KEY=needed APP_RECAPTCHA_SECRET=needed ###&lt; beelab/recaptcha2-bundle ### apcu缓存错误 config/packages/cache.yaml 文件里的system,使用的apcu,由于我的php没有装这个扩展导致缓存报错 framework: cache: app: cache.adapter.redis # 下面的这个system缓存 使用的是apcu。如果你没有安装apcu这个php扩展，可以替换成其他的 # 例如：cache.adapter.redis cache.adapter.filesystem 等等 system: cache.adapter.apcu default_redis_provider: snc_redis.cache pools: doctrine.cache: null prefix_seed: packagist recaptcha2的问题 查阅以下代码。 templates/registration/register.html.twig templates/reset_password/request.html.twig 我们会发现这个设置了一个set requiresRecaptcha = true，这个会让我们上面.env中所设置的APP_RECAPTCHA_ENABLED=false 无效。 这样就会导致如果你没有设置APP_RECAPTCHA_SITE_KEY和APP_RECAPTCHA_SECRET，那么你就将永远无法提交注册表单，和重置密码的表单。 这个问题困扰了我很久，具体表现为，点击注册按钮，没有任何反应，只有console控制台报了一下recaptcha.js的错误。 如果你跟我一样，并没有设置recaptcha,那么你需要删除上面两个代码文件中的{% set requiresRecaptcha = true %} 并且再次重新 composer install，或者手动运行php bin/console cache:clear 当你都已经准备好了之后，就可以使用composer install来安装你的程序了。 然后使用symfony serve 启动一个简单的webserver 如果你没有symfony,请到这里下载一个 https://symfony.com/download 提交自己的包 一旦你成功完成以上操作，成功注册了一个账号之后，你可以尝试提交自己的私有包。 例如 https://github.com/xxx/xxx，https://gitlab.com/xxx/xxx 但，需要记住的是： 这个链接对应的代码库应该是一个 pulic 公开的库,否则你需要到你部署packagist项目的机器上设置ssh 或者 设置 git clone https时的账号密码 而且你的代码库中，应该拥有composer.json文件，并且正确的配置composer.json内容。 一旦提交成功，你可以点击update按钮更新你的私有包，记住要在服务器上运行php bin/console packagist:run-workers 当然 最后的做法是按照文档上的，在服务器上使用crontab 运行上面的命令。 使用自己的包 一旦上面的都完成了，那么你就已经成功的提交了一个包到自己私有的packagist上了 在另一个项目中的composer.json中加上 自己的仓库 &quot;repositories&quot;: [ { &quot;type&quot; : &quot;composer&quot;, &quot;url&quot; : &quot;https://packagist.mydomain.com&quot; } ] 然后composer require my/packagist，你就可以成功的下载 配置私有的gitlab-domains 正常来说，我们的包应该拥有一个版本号，并且可以下载dist 但如果我们使用私有的gitlab,每次composer install 或者 composer require 时 是使用 git clone的方式去拉取源码。 你可以使用 composer require xxx/xxx -vvv的方式来查看composer 下载每个包的细节 这样就会对你composer install时的环境有要求，必须拥有git 命令。 在一些k8s,docker 镜像中install 时可能会出现一些些问题。 我们在packagist部署的机器上运行下面的这个命令，再次去packagist中更新自己的包 然后再rm -rf vendor &amp;&amp; rm -rf composer.lock &amp;&amp; composer install -vvv 你就可以看到以dist的方式去gitlab下载源码了，不再是git clone的方式去克隆源码 composer --global config gitlab-domains gitlab.xxx.cn gitlab.com ","link":"https://blog.marun.run/bu-shu-packagist-si-fu-da-pei-gitlab-si-fu-shi-yong/"},{"title":"gitlab-ci/cd","content":"简单介绍gitlab-ci/cd的使用 官方文档： GitLab CI/CD 没有比官方文档更好不过的教程了！ gitlab-ci如何运行？ gitlab-ci并不是平白无故运行起来的，依托于一个叫做GitLab Runner 的东西，我们配置的gitlab-ci 才会顺利的运行。 如果你对Install GitLab Runner 感兴趣，可以参看文档，gitlab-runner是如何安装的。 ci/cd的运行 至少需要一个gitlab-runner。当然也可以配置多个，比如您可以在macos系统上安装gitlab-runner并注册到gitlab中，这样您就可以将类似ios打包的过程放到这个runner上了。以此类推，如果您有特殊要求的ci/cd，可以指定特殊的runner。 ci/cd运行过程： gitlab按照您项目中的的gitlab-ci.yaml配置，创建一个或多个job,然后等待gitlab-runner运行这些job。 大概的过程类似于： 选择一个执行器 也就是执行命令等操作的环境，需要gitlab-runner中配置。例如shell也就是在gitlab-runner主机上运行。docker就是拉取某个镜像，在镜像中执行命令。 下文的runner的执行器均为docker 拉取源码 重建缓存 如果当前job或者流水线有配置缓存，会拉取缓存。（下文详细说） 下载artifacts 如果当前job的上游有配置artifacts，会下载artifacts（下文详细说） 执行命令 执行您的scripts CI/CD pipelines 流水线 顾名思义，流水线 ，也就是定义整个流程需要完成哪些阶段，每个阶段需要做什么。 Stages 阶段： 定义了整个pipelines需要做哪些事，这些事需要顺序执行。 例如：先下载依赖，然后编译代码，再推送编译后的代码，最后发一个ci/cd成功的通知 Jobs 作业： 每个阶段需要完成的操作，例如：使用某个php镜像，执行composer install 命令下载依赖。 以下为官方示例为例： stages: # 阶段 - build - test - deploy image: alpine # 作业 build_a: stage: build script: - echo &quot;This job builds something.&quot; build_b: stage: build script: - echo &quot;This job builds something else.&quot; test_a: stage: test script: - echo &quot;This job tests something. It will only run when all jobs in the&quot; - echo &quot;build stage are complete.&quot; test_b: stage: test script: - echo &quot;This job tests something else. It will only run when all jobs in the&quot; - echo &quot;build stage are complete too. It will start at about the same time as test_a.&quot; deploy_a: stage: deploy script: - echo &quot;This job deploys something. It will only run when all jobs in the&quot; - echo &quot;test stage complete.&quot; deploy_b: stage: deploy script: - echo &quot;This job deploys something else. It will only run when all jobs in the&quot; - echo &quot;test stage complete. It will start at about the same time as deploy_a.&quot; 对于更多内容，可参考官方文档 Jobs 作业 作业才是流水线每个阶段实际执行的内容，每个作业都应该定义在顶级，并且拥有至少一个script。 您可以理解为流水线的工人 以官方文档为例： job1: # 作业的名称 script: &quot;execute-script-for-job1&quot; # 作业执行的脚本 job2: # 作业的名称 script: &quot;execute-script-for-job2&quot; # 作业执行的脚本 作业定义的数量不受限制，每个作业都是独立的单独运行。 ⚠️ 每个作业都是单独，独立运行。所以，即使在同一个流水线中 也就是在一次ci/cd中。每个作业运行的runner都可能是不同的。 例如：你在runner1 上拉取了依赖，你在runner2上编译了代码，你在runner3上发送了一个通知。 那么，如果在deploy时想使用上一个阶段编译出来的文件该如何操作呢？ Caching / Artifacts 缓存和制品 Artifacts 制品，可以理解为作业产出的内容（上个流水线工人产出的内容） 简单以下面这个为例： stages: - install - build - deploy install-online: stage: install image: tool/yarn:v1 script: - yarn install - yarn build:prod # 将yarn build 打包出来的dist 作为制品，传到下一个job artifacts: paths: - dist/ # 默认情况下，后面阶段的作业会自动下载由前面阶段的作业创建的所有制品。 build-online-image: stage: build image: tool/docker:latest only: - master script: - docker build -t $ONLIE_IMAGE_NAME . - docker push $ONLIE_IMAGE_NAME - docker tag $ONLIE_IMAGE_NAME $IMAGE - docker push $IMAGE deploy-online: stage: deploy image: kubematrix/cli:v1 # 如果您不需要依赖任何的制品，你可以使用dependencies: [] dependencies: [] only: - master script: - matrix workloads deploy 在install时安装依赖并打包出一个dist文件夹，使用artifacts将这个文件夹打包出来。 在install执行完成后，build阶段时会自动下载之前的artifacts 也就是dist文件夹，完成了将上个阶段编译出来的文件带到下个阶段的操作。 dependencies 依赖 可参考官方文档， Cache 缓存 以php为例: # # https://gitlab.com/gitlab-org/gitlab/-/tree/master/lib/gitlab/ci/templates/PHP.gitlab-ci.yml # image: php:7.2 # Cache libraries in between jobs cache: key: $CI_COMMIT_REF_SLUG paths: - vendor/ before_script: # Install and run Composer - curl --show-error --silent &quot;https://getcomposer.org/installer&quot; | php - php composer.phar install test: script: - vendor/bin/phpunit --configuration phpunit.xml --coverage-text --colors=never 以$CI_COMMIT_REF_SLUG(预定义变量，这里是指分支的名称)为key， 将vendor/目录缓存下来。 缓存下来 以后的每一次作业，都会下载缓存，这样会加速composer install 的速度 ⚠️需要注意的点： cache:key 如果不指定key，那么默认同一个项目的所有build共享这个缓存。关于cache:key的各种场景使用，可参考：Caching in GitLab CI/CD | GitLab 默认cache使用本地磁盘保存，但这样会出现一个问题。如果每次job运行都在不同的runner上，那么缓存是不可以共享的。这时候就需要引入一个分布式存储这个缓存文件。 配置阿里云OSS存储cache文件 Advanced configuration | GitLab 为了统一缓存，我们需要使用OSS 首先进入gitlab-runner对应的主机，gitlab-runner list 查看配置文件存放的位置 /etc/gitlab-runner/config.toml [[runners]] name = &quot;cloud5&quot; url = &quot;https://gitlab.xxx.cn/&quot; token = &quot;token&quot; executor = &quot;docker&quot; [runners.custom_build_dir] [runners.cache] Type = &quot;s3&quot; Shared = true Path = &quot;xxx&quot; [runners.cache.s3] ServerAddress = &quot;oss-cn-hangzhou.aliyuncs.com&quot; AccessKey = &quot;key&quot; SecretKey = &quot;sec&quot; BucketName = &quot;gitlab-runner-cached&quot; BucketLocation = &quot;oss-cn-hangzhou&quot; [runners.docker] tls_verify = false image = &quot;ruby:2.6&quot; privileged = false disable_entrypoint_overwrite = false oom_kill_disable = false disable_cache = false volumes = [&quot;/var/run/docker.sock:/var/run/docker.sock&quot;, &quot;/root/.docker/:/root/.docker/&quot;, &quot;/data/gitlab-runner/cache:/cache&quot;, &quot;/data/gitlab-runner/npm-cache:/root/.npm&quot;, &quot;/data/gitlab-runner/composer-cache:/root/.composer&quot;] shm_size = 0 pull_policy = &quot;if-not-present&quot; 主要内容为以下： [runners.cache] Type = &quot;s3&quot; # 缓存类型 Shared = true # 缓存在多个runner之间共享 Path = &quot;xxx&quot; # 缓存路径前缀，如果一个gitlab-runner在多个gitlab中注册，可以使用前缀的方式做区分 [runners.cache.s3] ServerAddress = &quot;oss-cn-hangzhou.aliyuncs.com&quot; # oss地址 AccessKey = &quot;key&quot; # key SecretKey = &quot;sec&quot; # secret BucketName = &quot;gitlab-runner-cached&quot; # bucketName BucketLocation = &quot;oss-cn-hangzhou&quot; # BucketLocation 以此类推，你需要在所有的gitlab-runner中配置此缓存，这样所有的缓存都会被上传到阿里云的OSS中，无论你的job运行在哪个runner中。 示例： 以一个前端项目为例： # 定义 variables: ONLIE_IMAGE_NAME: registry.cn-hangzhou.aliyuncs.com/$CI_PROJECT_NAME:$CI_COMMIT_SHORT_SHA stages: - preInstall - install - build - deploy build-image: stage: build image: docker only: - test script: - docker build -t $IMAGE . - docker push $IMAGE # ---------------------------------- master ---------------------------------- # 只有当package.json变化时才会重新install preInstall-job: stage: preInstall image: yarn:v1 cache: key: $CI_COMMIT_REF_SLUG paths: - node_modules/ - yarn.lock # 只需要推送缓存，不需要拉取缓存 policy: push only: refs: - master changes: - package.json script: - yarn install install-online: stage: install image: yarn:v1 cache: key: $CI_COMMIT_REF_SLUG paths: - node_modules/ - yarn.lock # 只需要拉取缓存，不需要推送缓存 policy: pull only: - master script: # 当缓存存在时不需要install - if [ ! -d &quot;./node_modules/&quot; ];then yarn install; else ls -alh; fi - yarn build:prod # 打包制品给后面的job artifacts: paths: - dist/ build-online-image: stage: build # 使用docker镜像 image: docker:latest # 只在master分支变动时运行这个job only: - master script: - docker build -t $ONLIE_IMAGE_NAME . # 构建镜像 - docker push $ONLIE_IMAGE_NAME # 推送镜像 deploy-online: stage: deploy # 使用kubematrix-cli镜像 image: kubematrix/cli:v1 # 依赖为空，表示不需要任何的依赖 dependencies: [] only: - master script: - matrix workloads deploy 示例讲解 GitLab CI/CD variables | GitLab 变量 可以在文档Predefined variables reference | GitLab 中查看预定义的变量 首先定义变量 ONLIE_IMAGE_NAME 以供使用 定义stages 流水线阶段 定义了4个阶段，preInstall，install，build，deploy，下面的job将按照关联的stag顺序执行 定义job stag 定义了job对应的阶段。 image：定义了执行脚本所使用的docker镜像。 only ： 定义了job只在某些情况下执行。默认为only.refs输入分支名，或与分支相匹配的正则表达式。only.changes 检测只在某些文件变动时运行。 cache ： 定义了改job使用缓存，cache.policy 缓存规则，默认为pull-push开始job前拉取缓存,job成功后推送缓存。 可修改为push只推送，不拉取。pull只拉取，不推送 artifacts： 定义了需要打包的制品，给后面的job使用 dependencies : 需要的依赖，如果上面的job有artifacts 会默认下载，如果不需要artifacts 可将这个属性定义为空数组 ","link":"https://blog.marun.run/gitlab-cicd/"},{"title":"android逆向 360加固apk，frida Hook java代码","content":"参考文章： https://www.52pojie.cn/forum.php?mod=viewthread&amp;tid=1287307 书接上文，这次我会尝试apk的逆向，找到app不能抓包的原因 如图所示，这个app被360加固了，几经百度，按照开头那篇文章的参考，我们得到了dex文件 将多个dex文件 移动到电脑上 由于有多个dex文件，我们需要合并查看代码 jadx工具： https://github.com/skylot/jadx/ 具体代码如下 import os, sys # python3.7 merge_dex.py ./file/ livedex if __name__ == &quot;__main__&quot;: if len(sys.argv) &lt; 3 : print(&quot;start error&quot;) sys.exit() print(sys.argv[1], sys.argv[2]) path = sys.argv[1] #文件夹目录 files= os.listdir(path) #得到文件夹下的所有文件名称 s = [] for file in files: #遍历文件夹 if file.find(&quot;dex&quot;) &gt; 0: ## 查找dex 文件 sh = 'jadx.bat -j 1 -r -d ' + sys.argv[2] + &quot; &quot; + path + file print(sh) os.system(sh) 最终结果就是这样了，具体代码就不放了。 经过查看源码，发现是okHttp设置了no_proxy 只需要将这一部分代码hook掉就ok了。 我使用的frida ：https://github.com/frida/frida 电脑使用pip install // 电脑安装frida pip install frida-tools # CLI tools pip install frida # Python bindings // 将frida-server推到手机 adb.exe push .\\frida-server-14.0.7-android-arm64 /data/local/tmp/frida-server // 连接到手机 adb shell cd /data/local/tmp // 切到root用户 su // 修改权限 chmod 777 frida-server // 执行 ./frida-server 上面就将手机的frida-server开启了，然后在电脑上试试是否可以成功连接 frida-ps.exe -U 如果有显示手机相关进程就ok 将自己编写的js代码推到手机 进行 hook frida -U --no-pause -f com.showstartfans.activity -l .\\showstart.js showstart.js 代码: Java.perform( function () { var application = Java.use(&quot;android.app.Application&quot;); application.attach.overload('android.content.Context').implementation = function(context) { var result = this.attach(context); // 先执行原来的attach方法 var classloader = context.getClassLoader(); Java.classFactory.loader = classloader; var Hook_class = Java.classFactory.use(&quot;com.taihebase.activity.utils.SecurityUtil&quot;); console.log(&quot;Hook_class: &quot; + Hook_class); Hook_class.getNeedCapturePacket.implementation = function(){ return true; } return result; } } ) frida关于java的官方文档: https://frida.re/docs/javascript-api/#java 这个时候就可以使用fiddler安心的抓接口了 故技重施~ 又可以流畅的下单购票了。 ","link":"https://blog.marun.run/android-ni-xiang-360-jia-gu-apkfrida-hook-java-dai-ma/"},{"title":"fiddler-实战 ","content":"前言： 前段时间看了一档B站的综艺：《说唱新世代》，最近节目里面的选手开始巡演。想买票去看看的，在秀动上没买到票，很气。so，想想办法吧，于是有了以下记录。 一，尝试APP抓包看看 抓包工具：Fiddler 具体配置不再赘述，网上有很多文章都有讲 App配置wifi代理8888 很可惜，App并没有成功，只抓到几张图片的😭 二，尝试wap端 既然App端不可以的话，那就尝试一下wap端吧。 呐，这里显示只能去app购买，可恶😤，右键检查一下。 这里加入了一个id，扒下js，detail.js 这里判断了activity.type == 6 ，看下接口，哪里有type : 6的 呐，不出意外就是这里了，我们使用fillder 拦截response ，修改这个type 第一步，双击这玩意儿，开启response拦截功能 （注意这里会拦截所有的response，在没有抓到你想要的接口时，先点上面的go 放行所有的请求，直到你想要的接口出现） 第二步，搜到想要修改的值，我这里就是&quot;type&quot;:6 改为type:1 第三步，修改成功后点击这个run to xxxx的绿色按钮，再把第一步这里还原就行 好了，来看看现在的页面 从只能app购买，变成了立即购票 nice，兄弟。第一步完成了 我们点击立即购票 可惜这里显示即将开售，我们来看看这个地方的接口 这里我们故技重施，把这里的0 都改成10， 注意这里的salesStatus也需要改成1 呐，这里就可以直接点立即购买了,点进去就下单页了。 这里的已售罄其实也可以改，同样通过fiddler 改接口，我就不再演示了。 by the way , 这里如果直接进来的话，其实直接通过url下次再进来了，就不用前面的几步操作了，链接放下面 https://wap.showstart.com/pages/order/activity/confirm/confirm?sequence=123744&amp;ticketId=2d80ca0d806844cba6915518356f76c6&amp;ticketNum=1 ","link":"https://blog.marun.run/fiddler-shi-zhan-yi/"},{"title":"mysql中 group by排序，derived_merge优化的坑","content":"一个简单的表 CREATE TABLE `test` ( `id` int(11) NOT NULL AUTO_INCREMENT, `spu_id` int(11) DEFAULT NULL, `price` decimal(10,2) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci; 大概内容 id spu_id price 1 100 200 2 100 100 3 200 400 4 200 200 对spu_id进行分组，按price从小到大排序： SELECT * FROM `test` GROUP BY spu_id ORDER BY price 直接使用group by 查出来的数据是按id顺序分组的，并未达到预期 尝试使用子查询，先排序再分组 SELECT * FROM ( SELECT * FROM `test` ORDER BY price ) AS tmp GROUP BY spu_id 注意：这个方式在低版本中有效。在5.7版本中引入新特性 derived_merge优化过后无效了。 具体无效原因我们可使用explain 分析 EXPLAIN SELECT * FROM ( SELECT * FROM `test` ORDER BY price) AS tmp GROUP BY spu_id; 如图所示： MySQL 将子查询优化成了一个简单查询，子查询中的排序无效~ 解决方法： 将derived_merge 关闭 SET optimizer_switch='derived_merge=off'; SET GLOBAL optimizer_switch='derived_merge=off'; 使用特殊的查询阻止 derived_merge 优化 可以通过在子查询中使用任何阻止合并的构造来禁用合并，尽管这些构造对实现的影响不那么明显。防止合并的构造与派生表和视图引用相同： 聚合函数（SUM()， MIN()， MAX()， COUNT()，等等） DISTINCT GROUP BY HAVING LIMIT UNION 要么 UNION ALL 选择列表中的子查询 分配给用户变量 仅引用文字值（在这种情况下，没有基础表） 以上内容参考文档：mysql文档 那么我们可以将上面的那条sql语句修改为： SELECT * FROM ( SELECT * FROM `test` HAVING 1=1 ORDER BY price ) AS tmp GROUP BY spu_id; 使用 having 来阻止合并 那么再用explain看看 如有错误请指正~ 请多包涵 ","link":"https://blog.marun.run/mysql-group-by-pai-xu-derived_merge-you-hua-de-keng/"},{"title":" 在 Hyperf 框架中使用 prometheus + grafana 部署基本的监控","content":"参考： hyperf利用prometheus接入服务监控,使用grafana实现数据的实时监控显示 hyperf文档 本文章记录本人的第一次部署所踩的坑，未深入了解prometheus 和grafana 如有不当的地方请指正，谢谢！ 一. 使用docker-compose部署 version: '2' networks: monitor: driver: bridge services: prometheus: image: prom/prometheus container_name: prometheus hostname: prometheus restart: always volumes: # 将你的prometheus.yml文件放在当前文件同级下，或自定义 - ./prometheus.yml:/etc/prometheus/prometheus.yml #- /home/prometheus/node_down.yml:/etc/prometheus/node_down.yml ports: - &quot;9090:9090&quot; networks: monitor: ipv4_address: 172.18.0.3 grafana: image: grafana/grafana container_name: grafana hostname: grafana restart: always volumes: # 创建 etc目录，data目录存储grafana的数据 - ./etc:/etc/grafana - ./data:/var/lib/grafana ports: - &quot;3000:3000&quot; networks: monitor: ipv4_address: 172.18.0.4 node-exporter: image: prom/node-exporter container_name: node-exporter hostname: node-exporter restart: always ports: - &quot;9100:9100&quot; networks: monitor: ipv4_address: 172.18.0.2 注意：为了避免每次docker-compose 启动之后 ip会发生变化，我这里配置了固定IP，请根据个人实际情况配置，或参阅docker相关文档 使用命令docker-compose up启动容器 二. 项目配置 因为对 prometheus的不了解，我直接使用hyperf默认配置 引入组件 composer require hyperf/metric 发布默认配置文件 php bin/hyperf.php vendor:publish hyperf/metric 在config/autoload/dependencies.php中添加对应的Redis存储 return [ \\Prometheus\\Storage\\Adapter::class =&gt; \\Hyperf\\Metric\\Adapter\\Prometheus\\RedisStorageFactory::class, ]; 在上面的第一篇文章中，老哥说使用swoole_table更高效，我还不知道如何使用，有兴趣的老哥可以自己研究一下。 增加中间件 在config/autoload/middlewares.php文件中增加对应的中间件 return [ 'http' =&gt; [ \\Hyperf\\Metric\\Middleware\\MetricMiddleware::class, ], ]; 添加 metrics路由 Router::get('/metrics', function(){ $registry = Hyperf\\Utils\\ApplicationContext::getContainer()-&gt;get(Prometheus\\CollectorRegistry::class); $renderer = new Prometheus\\RenderTextFormat(); return $renderer-&gt;render($registry-&gt;getMetricFamilySamples()); }); 这样对项目的配置就完成了 三. prometheus的配置 在 prometheus.yml文件中增加对应的配置 scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: ['localhost:9090'] - job_name: 'node' # 注意这里的IP需要填写 node-exporter 容器的ip static_configs: - targets: ['172.18.0.2:9100'] - job_name: 'skeleton' # 这里填写的是宿主机的ip static_configs: - targets: ['10.0.75.1:9502'] 配置完成之后，再次 dokcer-compose up 访问 http://localhost:9090 查看 prometheus 如图所示，node 和 skeleton 都已启动 四. Grafana 配置 上面都配置完了，开始配置 Grafana 打开 http://localhost:3000 默认密码是: admin/admin 新建datasource 左侧边栏 add datasources 选择Prometheus 配置 datasource 填写容器的IP:端口 导入hyperf官方的JSON文件 导入之后需要将默认的 app_name改成你自己的 如：admin-api 就需要填写admin_api 改成下划线形式 查看监控 在Home中你就可以看到了 点进去查看 到此结束，小白第一次配置监控，还有很多东西没弄清楚 ","link":"https://blog.marun.run/hyperf-grafana/"}]}