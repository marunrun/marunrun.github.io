{"posts":[{"title":"gitlab-ci/cd","content":"简单介绍gitlab-ci/cd的使用 官方文档： GitLab CI/CD 没有比官方文档更好不过的教程了！ gitlab-ci如何运行？ gitlab-ci并不是平白无故运行起来的，依托于一个叫做GitLab Runner 的东西，我们配置的gitlab-ci 才会顺利的运行。 如果你对Install GitLab Runner 感兴趣，可以参看文档，gitlab-runner是如何安装的。 ci/cd的运行 至少需要一个gitlab-runner。当然也可以配置多个，比如您可以在macos系统上安装gitlab-runner并注册到gitlab中，这样您就可以将类似ios打包的过程放到这个runner上了。以此类推，如果您有特殊要求的ci/cd，可以指定特殊的runner。 ci/cd运行过程： gitlab按照您项目中的的gitlab-ci.yaml配置，创建一个或多个job,然后等待gitlab-runner运行这些job。 大概的过程类似于： 选择一个执行器 也就是执行命令等操作的环境，需要gitlab-runner中配置。例如shell也就是在gitlab-runner主机上运行。docker就是拉取某个镜像，在镜像中执行命令。 下文的runner的执行器均为docker 拉取源码 重建缓存 如果当前job或者流水线有配置缓存，会拉取缓存。（下文详细说） 下载artifacts 如果当前job的上游有配置artifacts，会下载artifacts（下文详细说） 执行命令 执行您的scripts CI/CD pipelines 流水线 顾名思义，流水线 ，也就是定义整个流程需要完成哪些阶段，每个阶段需要做什么。 Stages 阶段： 定义了整个pipelines需要做哪些事，这些事需要顺序执行。 例如：先下载依赖，然后编译代码，再推送编译后的代码，最后发一个ci/cd成功的通知 Jobs 作业： 每个阶段需要完成的操作，例如：使用某个php镜像，执行composer install 命令下载依赖。 以下为官方示例为例： stages: # 阶段 - build - test - deploy image: alpine # 作业 build_a: stage: build script: - echo &quot;This job builds something.&quot; build_b: stage: build script: - echo &quot;This job builds something else.&quot; test_a: stage: test script: - echo &quot;This job tests something. It will only run when all jobs in the&quot; - echo &quot;build stage are complete.&quot; test_b: stage: test script: - echo &quot;This job tests something else. It will only run when all jobs in the&quot; - echo &quot;build stage are complete too. It will start at about the same time as test_a.&quot; deploy_a: stage: deploy script: - echo &quot;This job deploys something. It will only run when all jobs in the&quot; - echo &quot;test stage complete.&quot; deploy_b: stage: deploy script: - echo &quot;This job deploys something else. It will only run when all jobs in the&quot; - echo &quot;test stage complete. It will start at about the same time as deploy_a.&quot; 对于更多内容，可参考官方文档 Jobs 作业 作业才是流水线每个阶段实际执行的内容，每个作业都应该定义在顶级，并且拥有至少一个script。 您可以理解为流水线的工人 以官方文档为例： job1: # 作业的名称 script: &quot;execute-script-for-job1&quot; # 作业执行的脚本 job2: # 作业的名称 script: &quot;execute-script-for-job2&quot; # 作业执行的脚本 作业定义的数量不受限制，每个作业都是独立的单独运行。 ⚠️ 每个作业都是单独，独立运行。所以，即使在同一个流水线中 也就是在一次ci/cd中。每个作业运行的runner都可能是不同的。 例如：你在runner1 上拉取了依赖，你在runner2上编译了代码，你在runner3上发送了一个通知。 那么，如果在deploy时想使用上一个阶段编译出来的文件该如何操作呢？ Caching / Artifacts 缓存和制品 Artifacts 制品，可以理解为作业产出的内容（上个流水线工人产出的内容） 简单以下面这个为例： stages: - install - build - deploy install-online: stage: install image: tool/yarn:v1 script: - yarn install - yarn build:prod # 将yarn build 打包出来的dist 作为制品，传到下一个job artifacts: paths: - dist/ # 默认情况下，后面阶段的作业会自动下载由前面阶段的作业创建的所有制品。 build-online-image: stage: build image: tool/docker:latest only: - master script: - docker build -t $ONLIE_IMAGE_NAME . - docker push $ONLIE_IMAGE_NAME - docker tag $ONLIE_IMAGE_NAME $IMAGE - docker push $IMAGE deploy-online: stage: deploy image: kubematrix/cli:v1 # 如果您不需要依赖任何的制品，你可以使用dependencies: [] dependencies: [] only: - master script: - matrix workloads deploy 在install时安装依赖并打包出一个dist文件夹，使用artifacts将这个文件夹打包出来。 在install执行完成后，build阶段时会自动下载之前的artifacts 也就是dist文件夹，完成了将上个阶段编译出来的文件带到下个阶段的操作。 dependencies 依赖 可参考官方文档， Cache 缓存 以php为例: # # https://gitlab.com/gitlab-org/gitlab/-/tree/master/lib/gitlab/ci/templates/PHP.gitlab-ci.yml # image: php:7.2 # Cache libraries in between jobs cache: key: $CI_COMMIT_REF_SLUG paths: - vendor/ before_script: # Install and run Composer - curl --show-error --silent &quot;https://getcomposer.org/installer&quot; | php - php composer.phar install test: script: - vendor/bin/phpunit --configuration phpunit.xml --coverage-text --colors=never 以$CI_COMMIT_REF_SLUG(预定义变量，这里是指分支的名称)为key， 将vendor/目录缓存下来。 缓存下来 以后的每一次作业，都会下载缓存，这样会加速composer install 的速度 ⚠️需要注意的点： cache:key 如果不指定key，那么默认同一个项目的所有build共享这个缓存。关于cache:key的各种场景使用，可参考：Caching in GitLab CI/CD | GitLab 默认cache使用本地磁盘保存，但这样会出现一个问题。如果每次job运行都在不同的runner上，那么缓存是不可以共享的。这时候就需要引入一个分布式存储这个缓存文件。 配置阿里云OSS存储cache文件 Advanced configuration | GitLab 为了统一缓存，我们需要使用OSS 首先进入gitlab-runner对应的主机，gitlab-runner list 查看配置文件存放的位置 /etc/gitlab-runner/config.toml [[runners]] name = &quot;cloud5&quot; url = &quot;https://gitlab.xxx.cn/&quot; token = &quot;token&quot; executor = &quot;docker&quot; [runners.custom_build_dir] [runners.cache] Type = &quot;s3&quot; Shared = true Path = &quot;xxx&quot; [runners.cache.s3] ServerAddress = &quot;oss-cn-hangzhou.aliyuncs.com&quot; AccessKey = &quot;key&quot; SecretKey = &quot;sec&quot; BucketName = &quot;gitlab-runner-cached&quot; BucketLocation = &quot;oss-cn-hangzhou&quot; [runners.docker] tls_verify = false image = &quot;ruby:2.6&quot; privileged = false disable_entrypoint_overwrite = false oom_kill_disable = false disable_cache = false volumes = [&quot;/var/run/docker.sock:/var/run/docker.sock&quot;, &quot;/root/.docker/:/root/.docker/&quot;, &quot;/data/gitlab-runner/cache:/cache&quot;, &quot;/data/gitlab-runner/npm-cache:/root/.npm&quot;, &quot;/data/gitlab-runner/composer-cache:/root/.composer&quot;] shm_size = 0 pull_policy = &quot;if-not-present&quot; 主要内容为以下： [runners.cache] Type = &quot;s3&quot; # 缓存类型 Shared = true # 缓存在多个runner之间共享 Path = &quot;xxx&quot; # 缓存路径前缀，如果一个gitlab-runner在多个gitlab中注册，可以使用前缀的方式做区分 [runners.cache.s3] ServerAddress = &quot;oss-cn-hangzhou.aliyuncs.com&quot; # oss地址 AccessKey = &quot;key&quot; # key SecretKey = &quot;sec&quot; # secret BucketName = &quot;gitlab-runner-cached&quot; # bucketName BucketLocation = &quot;oss-cn-hangzhou&quot; # BucketLocation 以此类推，你需要在所有的gitlab-runner中配置此缓存，这样所有的缓存都会被上传到阿里云的OSS中，无论你的job运行在哪个runner中。 示例： 以一个前端项目为例： # 定义 variables: ONLIE_IMAGE_NAME: registry.cn-hangzhou.aliyuncs.com/$CI_PROJECT_NAME:$CI_COMMIT_SHORT_SHA stages: - preInstall - install - build - deploy build-image: stage: build image: docker only: - test script: - docker build -t $IMAGE . - docker push $IMAGE # ---------------------------------- master ---------------------------------- # 只有当package.json变化时才会重新install preInstall-job: stage: preInstall image: yarn:v1 cache: key: $CI_COMMIT_REF_SLUG paths: - node_modules/ - yarn.lock # 只需要推送缓存，不需要拉取缓存 policy: push only: refs: - master changes: - package.json script: - yarn install install-online: stage: install image: yarn:v1 cache: key: $CI_COMMIT_REF_SLUG paths: - node_modules/ - yarn.lock # 只需要拉取缓存，不需要推送缓存 policy: pull only: - master script: # 当缓存存在时不需要install - if [ ! -d &quot;./node_modules/&quot; ];then yarn install; else ls -alh; fi - yarn build:prod # 打包制品给后面的job artifacts: paths: - dist/ build-online-image: stage: build # 使用docker镜像 image: docker:latest # 只在master分支变动时运行这个job only: - master script: - docker build -t $ONLIE_IMAGE_NAME . # 构建镜像 - docker push $ONLIE_IMAGE_NAME # 推送镜像 deploy-online: stage: deploy # 使用kubematrix-cli镜像 image: kubematrix/cli:v1 # 依赖为空，表示不需要任何的依赖 dependencies: [] only: - master script: - matrix workloads deploy 示例讲解 GitLab CI/CD variables | GitLab 变量 可以在文档Predefined variables reference | GitLab 中查看预定义的变量 首先定义变量 ONLIE_IMAGE_NAME 以供使用 定义stages 流水线阶段 定义了4个阶段，preInstall，install，build，deploy，下面的job将按照关联的stag顺序执行 定义job stag 定义了job对应的阶段。 image：定义了执行脚本所使用的docker镜像。 only ： 定义了job只在某些情况下执行。默认为only.refs输入分支名，或与分支相匹配的正则表达式。only.changes 检测只在某些文件变动时运行。 cache ： 定义了改job使用缓存，cache.policy 缓存规则，默认为pull-push开始job前拉取缓存,job成功后推送缓存。 可修改为push只推送，不拉取。pull只拉取，不推送 artifacts： 定义了需要打包的制品，给后面的job使用 dependencies : 需要的依赖，如果上面的job有artifacts 会默认下载，如果不需要artifacts 可将这个属性定义为空数组 ","link":"https://marunrun.github.io/gitlab-cicd/"},{"title":"android逆向 360加固apk，frida Hook java代码","content":"参考文章： https://www.52pojie.cn/forum.php?mod=viewthread&amp;tid=1287307 书接上文，这次我会尝试apk的逆向，找到app不能抓包的原因 如图所示，这个app被360加固了，几经百度，按照开头那篇文章的参考，我们得到了dex文件 将多个dex文件 移动到电脑上 由于有多个dex文件，我们需要合并查看代码 jadx工具： https://github.com/skylot/jadx/ 具体代码如下 import os, sys # python3.7 merge_dex.py ./file/ livedex if __name__ == &quot;__main__&quot;: if len(sys.argv) &lt; 3 : print(&quot;start error&quot;) sys.exit() print(sys.argv[1], sys.argv[2]) path = sys.argv[1] #文件夹目录 files= os.listdir(path) #得到文件夹下的所有文件名称 s = [] for file in files: #遍历文件夹 if file.find(&quot;dex&quot;) &gt; 0: ## 查找dex 文件 sh = 'jadx.bat -j 1 -r -d ' + sys.argv[2] + &quot; &quot; + path + file print(sh) os.system(sh) 最终结果就是这样了，具体代码就不放了。 经过查看源码，发现是okHttp设置了no_proxy 只需要将这一部分代码hook掉就ok了。 我使用的frida ：https://github.com/frida/frida 电脑使用pip install // 电脑安装frida pip install frida-tools # CLI tools pip install frida # Python bindings // 将frida-server推到手机 adb.exe push .\\frida-server-14.0.7-android-arm64 /data/local/tmp/frida-server // 连接到手机 adb shell cd /data/local/tmp // 切到root用户 su // 修改权限 chmod 777 frida-server // 执行 ./frida-server 上面就将手机的frida-server开启了，然后在电脑上试试是否可以成功连接 frida-ps.exe -U 如果有显示手机相关进程就ok 将自己编写的js代码推到手机 进行 hook frida -U --no-pause -f com.showstartfans.activity -l .\\showstart.js showstart.js 代码: Java.perform( function () { var application = Java.use(&quot;android.app.Application&quot;); application.attach.overload('android.content.Context').implementation = function(context) { var result = this.attach(context); // 先执行原来的attach方法 var classloader = context.getClassLoader(); Java.classFactory.loader = classloader; var Hook_class = Java.classFactory.use(&quot;com.taihebase.activity.utils.SecurityUtil&quot;); console.log(&quot;Hook_class: &quot; + Hook_class); Hook_class.getNeedCapturePacket.implementation = function(){ return true; } return result; } } ) frida关于java的官方文档: https://frida.re/docs/javascript-api/#java 这个时候就可以使用fiddler安心的抓接口了 故技重施~ 又可以流畅的下单购票了。 ","link":"https://marunrun.github.io/android-ni-xiang-360-jia-gu-apkfrida-hook-java-dai-ma/"},{"title":"fiddler-实战 ","content":"前言： 前段时间看了一档B站的综艺：《说唱新世代》，最近节目里面的选手开始巡演。想买票去看看的，在秀动上没买到票，很气。so，想想办法吧，于是有了以下记录。 一，尝试APP抓包看看 抓包工具：Fiddler 具体配置不再赘述，网上有很多文章都有讲 App配置wifi代理8888 很可惜，App并没有成功，只抓到几张图片的😭 二，尝试wap端 既然App端不可以的话，那就尝试一下wap端吧。 呐，这里显示只能去app购买，可恶😤，右键检查一下。 这里加入了一个id，扒下js，detail.js 这里判断了activity.type == 6 ，看下接口，哪里有type : 6的 呐，不出意外就是这里了，我们使用fillder 拦截response ，修改这个type 第一步，双击这玩意儿，开启response拦截功能 （注意这里会拦截所有的response，在没有抓到你想要的接口时，先点上面的go 放行所有的请求，直到你想要的接口出现） 第二步，搜到想要修改的值，我这里就是&quot;type&quot;:6 改为type:1 第三步，修改成功后点击这个run to xxxx的绿色按钮，再把第一步这里还原就行 好了，来看看现在的页面 从只能app购买，变成了立即购票 nice，兄弟。第一步完成了 我们点击立即购票 可惜这里显示即将开售，我们来看看这个地方的接口 这里我们故技重施，把这里的0 都改成10， 注意这里的salesStatus也需要改成1 呐，这里就可以直接点立即购买了,点进去就下单页了。 这里的已售罄其实也可以改，同样通过fiddler 改接口，我就不再演示了。 by the way , 这里如果直接进来的话，其实直接通过url下次再进来了，就不用前面的几步操作了，链接放下面 https://wap.showstart.com/pages/order/activity/confirm/confirm?sequence=123744&amp;ticketId=2d80ca0d806844cba6915518356f76c6&amp;ticketNum=1 ","link":"https://marunrun.github.io/fiddler-shi-zhan-yi/"},{"title":"mysql中 group by排序，derived_merge优化的坑","content":"一个简单的表 CREATE TABLE `test` ( `id` int(11) NOT NULL AUTO_INCREMENT, `spu_id` int(11) DEFAULT NULL, `price` decimal(10,2) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci; 大概内容 id spu_id price 1 100 200 2 100 100 3 200 400 4 200 200 对spu_id进行分组，按price从小到大排序： SELECT * FROM `test` GROUP BY spu_id ORDER BY price 直接使用group by 查出来的数据是按id顺序分组的，并未达到预期 尝试使用子查询，先排序再分组 SELECT * FROM ( SELECT * FROM `test` ORDER BY price ) AS tmp GROUP BY spu_id 注意：这个方式在低版本中有效。在5.7版本中引入新特性 derived_merge优化过后无效了。 具体无效原因我们可使用explain 分析 EXPLAIN SELECT * FROM ( SELECT * FROM `test` ORDER BY price) AS tmp GROUP BY spu_id; 如图所示： MySQL 将子查询优化成了一个简单查询，子查询中的排序无效~ 解决方法： 将derived_merge 关闭 SET optimizer_switch='derived_merge=off'; SET GLOBAL optimizer_switch='derived_merge=off'; 使用特殊的查询阻止 derived_merge 优化 可以通过在子查询中使用任何阻止合并的构造来禁用合并，尽管这些构造对实现的影响不那么明显。防止合并的构造与派生表和视图引用相同： 聚合函数（SUM()， MIN()， MAX()， COUNT()，等等） DISTINCT GROUP BY HAVING LIMIT UNION 要么 UNION ALL 选择列表中的子查询 分配给用户变量 仅引用文字值（在这种情况下，没有基础表） 以上内容参考文档：mysql文档 那么我们可以将上面的那条sql语句修改为： SELECT * FROM ( SELECT * FROM `test` HAVING 1=1 ORDER BY price ) AS tmp GROUP BY spu_id; 使用 having 来阻止合并 那么再用explain看看 如有错误请指正~ 请多包涵 ","link":"https://marunrun.github.io/mysql-group-by-pai-xu-derived_merge-you-hua-de-keng/"},{"title":" 在 Hyperf 框架中使用 prometheus + grafana 部署基本的监控","content":"参考： hyperf利用prometheus接入服务监控,使用grafana实现数据的实时监控显示 hyperf文档 本文章记录本人的第一次部署所踩的坑，未深入了解prometheus 和grafana 如有不当的地方请指正，谢谢！ 一. 使用docker-compose部署 version: '2' networks: monitor: driver: bridge services: prometheus: image: prom/prometheus container_name: prometheus hostname: prometheus restart: always volumes: # 将你的prometheus.yml文件放在当前文件同级下，或自定义 - ./prometheus.yml:/etc/prometheus/prometheus.yml #- /home/prometheus/node_down.yml:/etc/prometheus/node_down.yml ports: - &quot;9090:9090&quot; networks: monitor: ipv4_address: 172.18.0.3 grafana: image: grafana/grafana container_name: grafana hostname: grafana restart: always volumes: # 创建 etc目录，data目录存储grafana的数据 - ./etc:/etc/grafana - ./data:/var/lib/grafana ports: - &quot;3000:3000&quot; networks: monitor: ipv4_address: 172.18.0.4 node-exporter: image: prom/node-exporter container_name: node-exporter hostname: node-exporter restart: always ports: - &quot;9100:9100&quot; networks: monitor: ipv4_address: 172.18.0.2 注意：为了避免每次docker-compose 启动之后 ip会发生变化，我这里配置了固定IP，请根据个人实际情况配置，或参阅docker相关文档 使用命令docker-compose up启动容器 二. 项目配置 因为对 prometheus的不了解，我直接使用hyperf默认配置 引入组件 composer require hyperf/metric 发布默认配置文件 php bin/hyperf.php vendor:publish hyperf/metric 在config/autoload/dependencies.php中添加对应的Redis存储 return [ \\Prometheus\\Storage\\Adapter::class =&gt; \\Hyperf\\Metric\\Adapter\\Prometheus\\RedisStorageFactory::class, ]; 在上面的第一篇文章中，老哥说使用swoole_table更高效，我还不知道如何使用，有兴趣的老哥可以自己研究一下。 增加中间件 在config/autoload/middlewares.php文件中增加对应的中间件 return [ 'http' =&gt; [ \\Hyperf\\Metric\\Middleware\\MetricMiddleware::class, ], ]; 添加 metrics路由 Router::get('/metrics', function(){ $registry = Hyperf\\Utils\\ApplicationContext::getContainer()-&gt;get(Prometheus\\CollectorRegistry::class); $renderer = new Prometheus\\RenderTextFormat(); return $renderer-&gt;render($registry-&gt;getMetricFamilySamples()); }); 这样对项目的配置就完成了 三. prometheus的配置 在 prometheus.yml文件中增加对应的配置 scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: ['localhost:9090'] - job_name: 'node' # 注意这里的IP需要填写 node-exporter 容器的ip static_configs: - targets: ['172.18.0.2:9100'] - job_name: 'skeleton' # 这里填写的是宿主机的ip static_configs: - targets: ['10.0.75.1:9502'] 配置完成之后，再次 dokcer-compose up 访问 http://localhost:9090 查看 prometheus 如图所示，node 和 skeleton 都已启动 四. Grafana 配置 上面都配置完了，开始配置 Grafana 打开 http://localhost:3000 默认密码是: admin/admin 新建datasource 左侧边栏 add datasources 选择Prometheus 配置 datasource 填写容器的IP:端口 导入hyperf官方的JSON文件 导入之后需要将默认的 app_name改成你自己的 如：admin-api 就需要填写admin_api 改成下划线形式 查看监控 在Home中你就可以看到了 点进去查看 到此结束，小白第一次配置监控，还有很多东西没弄清楚 ","link":"https://marunrun.github.io/hyperf-grafana/"}]}