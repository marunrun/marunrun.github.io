{"posts":[{"title":"k8s operator 与webhook","content":"背景 上一次我们写了一个 xxl-job-agent，旨为在k8s的pod中运行一个sidecar注册执行器在xxl-job-admin，并可以进入业务容器执行命令。 后面发现每一次手动配置deploy，修改clusterroleBinding太麻烦，需要做到简单的自动化，所以这一次尝试使用Operator和Webhook来简化我们的操作。 Operator是什么 引用官方文档的一段话： operator 旨在记述（正在管理一个或一组服务的）运维人员的关键目标。 这些运维人员负责一些特定的应用和 Service，他们需要清楚地知道系统应该如何运行、如何部署以及出现问题时如何处理。 在 Kubernetes 上运行工作负载的人们都喜欢通过自动化来处理重复的任务。 Operator 模式会封装你编写的（Kubernetes 本身提供功能以外的）任务自动化代码。 可以理解为 Operator 是运维开发人员用来辅助自动化的一个工具，例如： 在一组sass服务中，以命名空间隔离，然后每个命名空间都需要一些特定的服务，可以使用Operator统一创建更新服务。 使用 CRD， 创建集群秘钥，在每一个命名空间中都存有统一的容器镜像秘钥，或其他通用配置。 webhook是什么 引用维基百科的解释：网页开发中的网络钩子（Webhook）是一种通过自定义回调函数来增加或更改网页表现的方法。这些回调可被可能与原始网站或应用相关的第三方用户及开发者保存、修改与管理 k8s中的webhook被官方称为准入 Webhook，引用官方解释： 准入 Webhook 是一种用于接收准入请求并对其进行处理的 HTTP 回调机制。 可以定义两种类型的准入 webhook，即 验证性质的准入 Webhook 和 修改性质的准入 Webhook。 修改性质的准入 Webhook 会先被调用。它们可以更改发送到 API 服务器的对象以执行自定义的设置默认值操作。 在完成了所有对象修改并且 API 服务器也验证了所传入的对象之后， 验证性质的 Webhook 会被调用，并通过拒绝请求的方式来强制实施自定义的策略。 编写Operator 在之前编写的xxl-job-agent 中，如果想要在新的命名空间中使用agent，每次都要给命名空间的default serviceAccount添加对应的clusterroleBinding，这次我们将使用Operator自动添加clusterrole。 快速编写operator可以使用https://book.kubebuilder.io/ mkdir xxl-agent-operator cd xxl-agent-operator &amp;&amp; kubebuilder init --domain xxxx.com --repo gitlab.xxx.cn/kubematrix/operators/xxl-agent-operator kubebuilder create api --group xxl-agent --version v1 --kind Pod 注意，这里我们不需要创建Resource，只需要创建Controller，Resource是CRD， 我们目前不需要CRD。 这里会生成一个controllers/pod_controller.go文件 // +kubebuilder:rbac:groups=core,resources=pods,verbs=get;list;watch;create;update;patch;delete // +kubebuilder:rbac:groups=core,resources=pods/status,verbs=get;update;patch // Reconcile is part of the main kubernetes reconciliation loop which aims to // move the current state of the cluster closer to the desired state. // the Pod object against the actual cluster state, and then // perform operations to make the cluster state reflect the state specified by // the user. // // For more details, check Reconcile and its Result here: // - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.10.0/pkg/reconcile func (r *PodReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) { ... } 修改 Reconcile上面的注解，监听pods的变动，一旦pods发生变动，就会通知到这里。 func (r *PodReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) { ctx := context.Background() r.log = ctrl.Log.WithName(&quot;PodController&quot;).WithValues(&quot;Pod&quot;, req.Name) // found the pod pod := &amp;v1.Pod{} if err := r.Client.Get(ctx, req.NamespacedName, pod); err != nil { return ctrl.Result{}, nil } // Ignore event of deleting pod if !pod.DeletionTimestamp.IsZero() { return ctrl.Result{}, nil } // get enable annotation enable, ok := pod.Annotations[constant.XXL_AGENT_ANNOTATION_ENABLE] if !ok { return ctrl.Result{}, nil } // parse annotation enable if enable, err := strconv.ParseBool(enable); !enable || err != nil { return ctrl.Result{}, nil } if err := r.syncClusterRole(ctx, pod); err != nil { return ctrl.Result{}, err } return ctrl.Result{}, nil } func (r *PodReconciler) syncClusterRole(ctx context.Context, pod *v1.Pod) error { role := &amp;rbacV1.ClusterRole{ ObjectMeta: metav1.ObjectMeta{ Name: constant.XXL_AGENT_EXEC_CLUSTER_ROLE, }, Rules: []rbacV1.PolicyRule{ { APIGroups: []string{&quot;&quot;}, Resources: []string{&quot;pods/exec&quot;}, Verbs: []string{&quot;create&quot;}, }, }, } // create role if err := r.Client.Get(ctx, client.ObjectKey{Name: constant.XXL_AGENT_EXEC_CLUSTER_ROLE}, role); err != nil { if errors.IsNotFound(err) { if err := r.Create(ctx, role); err != nil { return err } } } // async roleBinding roleBinding := &amp;rbacV1.ClusterRoleBinding{ ObjectMeta: metav1.ObjectMeta{Name: constant.XXL_AGENT_EXEC_CLUSTER_ROLE_BINDING}, RoleRef: rbacV1.RoleRef{ APIGroup: &quot;rbac.authorization.k8s.io&quot;, Kind: &quot;ClusterRole&quot;, Name: constant.XXL_AGENT_EXEC_CLUSTER_ROLE, }, Subjects: []rbacV1.Subject{}, } // get or create roleBinding if err := r.Client.Get(ctx, client.ObjectKey{Name: constant.XXL_AGENT_EXEC_CLUSTER_ROLE_BINDING}, roleBinding); err != nil { if errors.IsNotFound(err) { if err := r.Create(ctx, roleBinding); err != nil { return err } } } // check the roleBinding already has the pod namespace if res := utils.ArrFirst(roleBinding.Subjects, func(i rbacV1.Subject) bool { return i.Namespace == pod.Namespace &amp;&amp; i.Name == constant.XXL_AGENT_EXEC_SERVICE_ACCOUNT }); res == nil { origin := roleBinding.DeepCopy() roleBinding.Subjects = append(roleBinding.Subjects, rbacV1.Subject{ Kind: rbacV1.ServiceAccountKind, Name: constant.XXL_AGENT_EXEC_SERVICE_ACCOUNT, Namespace: pod.Namespace, }) desired, err := json.Marshal(roleBinding) if err != nil { return err } return r.Client.Patch(ctx, origin, client.RawPatch(types.MergePatchType, desired)) } return nil } 上面的代码也很简单，监听pods的变动，从pods的annotations中解析xxl-agent/enable注解，并且parse为布尔值，如果注解值是true，自动同步clusterrole。 笔者的理解是operator有点类似一个监听器，监听某些操作和变动，然后做出相应的调整，来保证服务或者工作负载能完好的运转。有点类似半自动化的k8s运维。 注意，这里我们不可以在operator中进行边车的注入，因为这个时候的pods已经准备好了，再进行边车注入会提示报错，需要在webhook中进行边车注入。 编写webhook 参考： https://github.com/kubernetes/kubernetes/blob/release-1.21/test/images/agnhost/webhook/main.go webhook 动态准入，有点类似过滤器 / 中间件，在完成某个资源的请求之前通过webhook过滤一遍，可以实现动态调整配置，拒绝某些请求。 编写一个webhook在pods创建的时候拦截过滤，这个时候pods还没有创建好，所以我们可以注入边车。 可以创建一个 webhook.yaml apiVersion: admissionregistration.k8s.io/v1 kind: MutatingWebhookConfiguration metadata: labels: app: xxl-agent-injector name: xxl-agent-injector webhooks: - admissionReviewVersions: - v1beta1 clientConfig: caBundle: ${CA_BUNDLE} service: name: xxl-agent-injector-webhook namespace: matrixio path: /inject port: 443 failurePolicy: Ignore matchPolicy: Exact name: xxl-agent-injector.duomai.com reinvocationPolicy: Never rules: - apiGroups: - &quot;&quot; apiVersions: - v1 operations: - CREATE resources: - pods scope: '*' sideEffects: None timeoutSeconds: 30 上面就是一个webhook的配置文件，解释一下一些比较关键的配置： rules 匹配请求-规则 rules可以是多个，每个rule都必须有一个或多个 operations、apiGroups、apiVersions 和 resources 以及资源的 scope operations 匹配的操作,CREATE、UPDATE、DELETE、CONNECT 或 * apiGroups 匹配的API组，“” 是核心（core）组 , &quot;*&quot; 是匹配所有 resources 是匹配的资源 “*”匹配所有资源，但不包括子资源 &quot;*/*&quot; 匹配所有资源，包括子资源。 &quot;pods/*&quot; 匹配 pod 的所有子资源。 &quot;*/status&quot; 匹配所有 status 子资源。 scope指定要匹配的范围。有效值为&quot;Cluster&quot;、&quot;Namespaced&quot; 和 &quot;*&quot;。 子资源匹配其父资源的范围。默认值为&quot;*&quot; &quot;Cluster&quot; 表示只有集群作用域的资源才能匹配此规则（API 对象 Namespace 是集群作用域的）。 &quot;Namespaced&quot; 意味着仅具有名字空间的资源才符合此规则。 &quot;*&quot; 表示没有作用域限制。 clientConfig 服务引用 当前匹配请求之后，请求将转发至某个服务，这个服务可以是内部，也可以是外部。内部服务使用service，外部服务使用url caBundle 根证书，可以使用export caBundle=$(kubectl get configmap -n kube-system extension-apiserver-authentication -o=jsonpath='{.data.client-ca-file}' | base64 | tr -d '\\n') 命令获取k8s集群的根证书 service 集群内部服务, 当选择了内部服务时，namespace 和name是必填的， port默认是443,path默认是/。 注意其中name是serviceName url 类似这种 https://xxx.xxx:443/inject url 所以上面那个配置文件中的rule意思就是，匹配所有pods的创建请求，在pods创建之前，请求matrixio命名空间下的xxl-agent-injector-webhook 服务， 请求的端口是443，路径 是/inject 注意：webhook服务必须是tls加密过的，这里的服务端证书可以使用脚本生成 ssl.sh 在使用脚本的时候注意修改自己的APP和 NAMESPACE, 如果使用本地url调试，可以手动修改 IP 至本机 ip 当一切都准备好之后，开始编写服务端的代码 main.go // 创建路由 router := mux.NewRouter() // 定义 Webhook 处理函数 router.HandleFunc(&quot;/inject&quot;, func(writer http.ResponseWriter, request *http.Request) { f := handler.Inject server(writer, request, admitHandler{ v1beta1: delegateV1beta1AdmitToV1(f), v1: f, }) }) // 启动 Webhook 服务 srv := &amp;http.Server{ Addr: &quot;:8443&quot;, Handler: router, } klog.Info(&quot;Webhook server is running...&quot;) if err := srv.ListenAndServeTLS(config.Options.Ssl.CertFile, config.Options.Ssl.KeyFile); err != nil { panic(err) } config.go package config import ( &quot;github.com/spf13/viper&quot; ) var Options OptionConfig func init() { if err := InitConfig(); err != nil { panic(err) } } func InitConfig() error { v := viper.New() v.SetConfigFile(&quot;config/application.yaml&quot;) if err := v.ReadInConfig(); err != nil { return err } return v.Unmarshal(&amp;Options) } type InjectConfig struct { SideCarImage string `json:&quot;sidecar-image&quot; mapstructure:&quot;sidecar-image&quot; ` } type SslConfig struct { CertFile string `json:&quot;cert&quot; mapstructure:&quot;cert&quot;` KeyFile string `json:&quot;key&quot; mapstructure:&quot;key&quot;` } type OptionConfig struct { Ssl SslConfig `json:&quot;ssl&quot; mapstructure:&quot;ssl&quot;` Inject InjectConfig `json:&quot;inject&quot; mapstructure:&quot;inject&quot;` } 上面的代码就是，从config/application.yaml中读取边车镜像，证书文件路径，然后启动一个http服务，绑定inject路由。 然后就是自己的逻辑代码，inject-handler.go func Inject(ar v1.AdmissionReview) *v1.AdmissionResponse { raw := ar.Request.Object.Raw pod := corev1.Pod{} deserializer := Codecs.UniversalDeserializer() // parse pods if _, _, err := deserializer.Decode(raw, nil, &amp;pod); err != nil { klog.Error(err) return utils.ToV1AdmissionResponse(err) } klog.Infof(&quot;admitting pod %s&quot;, pod.GenerateName) // get enable annotation enable, ok := pod.Annotations[constant.XXL_AGENT_ANNOTATION_ENABLE] if !ok { return nil } // parse annotation enable if enable, err := strconv.ParseBool(enable); !enable || err != nil { return nil } // 解析注解 agentOpt, err := parseAnnotation(&amp;pod) if err != nil { return nil } patch, err := createPatch(agentOpt) if err != nil { return utils.ToV1AdmissionResponse(err) } klog.Infof(&quot;success patch pod %s, %s&quot;, pod.GenerateName, patch) // 构造 AdmissionResponse 对象 uid := string(uuid.NewUUID()) pt := v1.PatchTypeJSONPatch return &amp;v1.AdmissionResponse{ UID: types.UID(uid), Allowed: true, Patch: patch, PatchType: &amp;pt, } } // parseAnnotation parse pod annotation func parseAnnotation(pod *corev1.Pod) (agentOpt *options.AgentOptions, err error) { annotations := []string{ constant.XXL_AGENT_JOB_ADDR, constant.XXL_AGENT_TARGET_CONTAINER, constant.XXL_AGENT_JOB_NAME, } agentOpt = &amp;options.AgentOptions{} for _, annotation := range annotations { res, ok := pod.GetAnnotations()[annotation] if !ok { return nil, errors.PodAnnotationNotFoundError{Annotation: annotation} } if annotation == constant.XXL_AGENT_JOB_ADDR { agentOpt.XxlJobAdminAddr = res } if annotation == constant.XXL_AGENT_TARGET_CONTAINER { agentOpt.TargetContainer = res } if annotation == constant.XXL_AGENT_JOB_NAME { agentOpt.JobName = res } } return } // createPatch func createPatch(agentOpt *options.AgentOptions) ([]byte, error) { value := corev1.Container{ Name: constant.SIDE_CAR_CONTAINER_NAME, Image: config.Options.Inject.SideCarImage, Ports: []corev1.ContainerPort{ { Name: &quot;http-xxl-agent&quot;, ContainerPort: 9999, }, }, Env: []corev1.EnvVar{ { Name: agentConst.CONTAINER_NAME, Value: agentOpt.TargetContainer, }, { Name: agentConst.XXL_JOB_NAME, Value: agentOpt.JobName, }, { Name: agentConst.XXL_JOB_ADDR, Value: agentOpt.XxlJobAdminAddr, }, { Name: agentConst.NAMESPACE, ValueFrom: &amp;corev1.EnvVarSource{ FieldRef: &amp;corev1.ObjectFieldSelector{ APIVersion: &quot;v1&quot;, FieldPath: &quot;metadata.namespace&quot;, }, }, }, { Name: agentConst.POD_NAME, ValueFrom: &amp;corev1.EnvVarSource{ FieldRef: &amp;corev1.ObjectFieldSelector{ APIVersion: &quot;v1&quot;, FieldPath: &quot;metadata.name&quot;, }, }, }, }, } // add a container to the pods return json.Marshal([]map[string]interface{}{ { &quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/containers/-&quot;, &quot;value&quot;: value, }, }) } 上面这段代码就是从请求中解析出pods， 然后从pods的注解中解析出配置，根据配置来生成 container ，然后生成对应的响应。 关于响应，请参考 https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/#response 这里编写webhook的时候请注意，response的allow值一定要是true，不然所有的pods更新都会被拒绝。当然 如果你确实有拒绝pods更新的需求，也可以是false 然后patch 是多维数组 [[&quot;op&quot; : &quot;add&quot;,&quot;path&quot; :&quot;,&quot;value&quot;:&quot;&quot; ],[...] ] 所有的operator，webhook 都建议在本地调试好之后再去集群中部署。 ","link":"https://blog.marun.run/k8s-operator-yu-webhook/"},{"title":"K8s 内网穿透 nps","content":"背景 公司内部机房k8s集群用作测试环境，只能内网访问。但对接某些第三方应用，测试的时候需要公网的回调地址。 Nps &amp; Npc ehang-io/nps 启动 (ehang-io.github.io) 内网穿透工具，找一台有公网ip的机器，按照文档把server端(nps)部署一下 然后dns解析到这台机器上，例如：*.proxy.marun.run 部署好之后，点这个客户端列表 新增，默认配置就ok 新增之后，点击列表最左边的加号，可以看到客户端命令。 因为我们需要访问k8s内网服务，所以我们在k8s内部署一套client(npc) Dockerfile 我直接用的nps/Dockerfile.npc (github.com)的这个，可以根据自己的需要自己打包镜像，然后传到自己的私有镜像仓库 npc-deploy.yaml kind: Deployment apiVersion: apps/v1 metadata: name: npc namespace: nps spec: replicas: 1 selector: matchLabels: app: npc template: metadata: labels: app: npc spec: containers: - name: npc image: 'marunrun/npc:latest' args: - '-server=127.0.0.1:8024' - '-vkey=1234' - '-type=tcp' 记得把args里的server和vkey值 换成你自己的。 然后去nps后台，添加一个域名解析 主机就是你解析到nps服务主机的域名，目标就是k8s内部服务的dns地址，记得加上端口号 自己访问试试就ok了，其他的配置啥的看看文档就可以了，目前这样的配置已经能解决我的问题了 ","link":"https://blog.marun.run/k8s-nei-wang-chuan-tou-nps/"},{"title":"xxl-job sidecar 运行在K8s中","content":"业务背景 公司有很多php的定时任务 以cronjob的方式运行在k8s集群内，大概有100多个，每一个任务运行都会创建一个job 使用kubectl get pods可以看到许多已完成的pod， 这些pod 默认不会删除，是会占用资源的 实际上可以通过设置保留完成 Job 数 successfulJobsHistoryLimit: 0 来删除已完成的pod 但还是会有一批定时任务同时创建的问题，例如某个业务 每两分钟运行一个定时任务，每四分钟再运行一个定时任务。一旦同时在更新部署其他业务，会造成资源抢占，（阿里云集群每个节点只能运行110个pod）也会造成一定的资源浪费。 实际上，这两个定时任务可以在同一个pod内执行，并没有必要启动两个pod来运行。 解决方案 首先想到的是使用xxl-job 或类似的定时任务调度中心来完成，但php接入xxl-job 又比较麻烦，对历史项目进行改造的成本很高。 一番思索加上google了一下，看到了阿里云的解决方案Sidecar方式接入SchedulerX (aliyun.com)，好像很适合目前的项目。 很nice，再github上找找，选择了golang版本的xxl-job/xxl-job-executor-go: xxl-job 执行器（golang 客户端） (github.com) golang更简单，打包出来占用内存 代码 执行器代码比较少，可以参考xxl-job-executor-go/example at master · xxl-job/xxl-job-executor-go (github.com) 这里我只写了一个k8s_exec.go package task import ( &quot;context&quot; &quot;fmt&quot; &quot;github.com/xxl-job/xxl-job-executor-go&quot; v1 &quot;k8s.io/api/core/v1&quot; &quot;k8s.io/client-go/kubernetes&quot; &quot;k8s.io/client-go/kubernetes/scheme&quot; &quot;k8s.io/client-go/rest&quot; &quot;k8s.io/client-go/tools/clientcmd&quot; &quot;k8s.io/client-go/tools/remotecommand&quot; &quot;log&quot; &quot;os&quot; &quot;strconv&quot; ) var ( client *kubernetes.Clientset config *rest.Config err error namespace string podName string ) func init() { // 是否在集群内运行？ inCluster, _ := strconv.ParseBool(os.GetEnv(&quot;IN_CLUSTER&quot;)) if inCluster { // 使用集群内配置 config, err = rest.InClusterConfig() } else { // 默认使用本机的~/.kube/conf 配置 config, err = clientcmd.BuildConfigFromFlags(&quot;&quot;, clientcmd.RecommendedHomeFile) } if err != nil { panic(err.Error()) } client, err = kubernetes.NewForConfig(config) if err != nil { panic(err.Error()) } namespace = os.Getenv(&quot;NAMESPACE&quot;) podName = os.Getenv(&quot;POD_NAME&quot;) } func K8s_exec(cxt context.Context, param *xxl.RunReq) string { logger := log.New(os.Stdout, fmt.Sprintf(&quot;XXL-AGENT-K8S-EXEC [%d]&quot;, param.LogID), 0) logger.Println(&quot;开始执行任务&quot;) // 执行命令 cmd := []string{ &quot;sh&quot;, &quot;-c&quot;, param.ExecutorParams, } // 构建请求 通过namespace podName containerName 找到对应的业务容器 req := client.CoreV1().RESTClient().Post().Resource(&quot;pods&quot;).Name(podName). Namespace(namespace).SubResource(&quot;exec&quot;).Param(&quot;container&quot;, os.Getenv(&quot;CONTAINER_NAME&quot;)) option := &amp;v1.PodExecOptions{ Command: cmd, Stdin: true, Stdout: true, Stderr: true, TTY: false, } req.VersionedParams( option, scheme.ParameterCodec, ) exec, err := remotecommand.NewSPDYExecutor(config, &quot;POST&quot;, req.URL()) if err != nil { logger.Panic(err) } err = exec.Stream(remotecommand.StreamOptions{ Stdin: os.Stdin, Stdout: os.Stdout, Stderr: os.Stderr, Tty: false, }) if err != nil { logger.Panic(err) } logger.Println(&quot;任务执行完毕&quot;) return &quot;executed&quot; } 其实就是通过namespace podName containerName 找到对应的业务容器（需要执行命令的容器），通过exec的方式执行对应的命令 然后在main.go注册一下 ... exec.RegTask(&quot;task.panic&quot;, task.Panic) ... 部署 注意这里，需要给ServiceAccount 添加对应的pods/exec权限 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: xxl-job-agent-exec rules: - apiGroups: - &quot;&quot; resources: - 'pods/exec' verbs: - create --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: xxl-job-agent-rolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: xxl-job-agent-exec subjects: - kind: ServiceAccount name: default namespace: default 然后弄个项目测试一下 kind: Deployment apiVersion: apps/v1 metadata: name: xxl-job-test namespace: contract labels: app: xxl-job-test app.auth.matrix.io/id: contract app.kubernetes.io/name: contract app.kubernetes.io/version: v1 version: v1 spec: replicas: 1 selector: matchLabels: app: xxl-job-test template: metadata: labels: app: xxl-job-test app.auth.matrix.io/id: contract app.kubernetes.io/name: contract app.kubernetes.io/version: v1 version: v1 spec: containers: - name: business-api image: 'yourDockerImage:lastest' ports: - name: http-80 containerPort: 80 protocol: TCP resources: {} imagePullPolicy: Always - name: xxl-job-agent image: 'xxl-job-agent:test' ports: - name: http containerPort: 9999 protocol: TCP env: - name: XXL_JOB_ADDR value: 'http://xxl-job.company.svc.cluster.local:8080/xxl-job-admin' - name: XXL_JOB_NAME value: contract-job - name: CONTAINER_NAME value: business-api - name: NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name 这里的deploy 中有两个container 其中 business-api是业务容器，xxl-job-agent就是我们的执行器 这里在执行器里需要配置一些环境变量 - name: XXL_JOB_ADDR # xxl-job 集群内部地址 按需配置 value: 'http://xxl-job.company.svc.cluster.local:8080/xxl-job-admin' - name: XXL_JOB_NAME # 执行器的名称 value: busiess-job - name: CONTAINER_NAME # 业务容器的名称 value: business-api - name: NAMESPACE # 命名空间 valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: POD_NAME # podName valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name 配置 新增执行器 这里的AppName 就是上面环境变量的XXL_JOB_NAME 如果没啥问题的话，执行器那里就能看到机器地址了 增加任务 这里运行模式选择BEAN JobHandler 就是上面main.go注册的 k8s.exec 任务参数：需要运行的命令，例如上面的php -v 实际上应该是你需要在业务容器内运行的命令 手动执行一次，能看到输出就说明没啥问题了 这样只用给需要定时任务的deploy加上agent ，然后再去xxl-job里加上执行器，任务就可以了。这样不会再有多余的pod，每次定时任务都是在业务容器内去执行了。 github 代码 xxl-job/xxl-job-executor-go: xxl-job 执行器（golang 客户端） (github.com) 分布式任务调度平台XXL-JOB (xuxueli.com) ","link":"https://blog.marun.run/xxl-job-sidecar-yun-xing-zai-k8s-zhong/"},{"title":"k8s安装kafka  kafka-ui ","content":"k8s安装kafka 并开放外部访问 参考文档： kafka 19.1.3 · bitnami/bitnami (artifacthub.io) charts/values.yaml at main · bitnami/charts (github.com) 添加 helm repo helm bitnami add bitnami https://charts.bitnami.com/bitnami 如果使用PV 请提前配置好storageclass 使用以下命令查看 kubectl get storageclass 建议使用values.yaml进行配置 kafka 默认配置参考：charts/values.yaml at main · bitnami/charts (github.com) 对应的zookeeper默认配置 charts/bitnami/zookeeper at main · bitnami/charts (github.com) 如果使用PV 需要配置权限,不然会出现没有权限的情况 volumePermissions: enabled: true zookeeper: volumePermissions: enabled: true 如需使用外网访问请参考文档charts/bitnami/kafka at main · bitnami/charts (github.com)添加配置 我这里使用nodePort方式： externalAccess: enabled: true service: type: &quot;NodePort&quot; autoDiscovery: enabled: true serviceAccount: create: true rbac: create: true 注意这里需要创建额外的svc 以我这里为例，三个cluster 需要创建三个svc，kafka-0-external kafka-1-external kafka-2-external kind: Service apiVersion: v1 metadata: name: kafka-{num}-external namespace: database spec: ports: - name: port protocol: TCP port: 9094 targetPort: 9094 selector: statefulset.kubernetes.io/pod-name: kafka-{num} type: NodePort sessionAffinity: None externalTrafficPolicy: Cluster 这样运行起来，就可以通过nodeip:nodeport 来访问k8s内的kafka服务了 $&gt; kubectl get svc -n database kafka-0-external NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kafka-0-external NodePort 10.103.255.3 &lt;none&gt; 9094:32402/TCP 19h 以上面的这个为例，我们的节点ip是：192.168.1.3 那么在k8s 外部，就可以使用192.168.1.3:32402 以此类推，其他两个cluster也可以这样访问，不过是nodeport不一样而已 ❯ kubectl get svc -n database NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kafka-0-external NodePort 10.103.255.3 &lt;none&gt; 9094:32402/TCP 19h kafka-1-external NodePort 10.106.84.112 &lt;none&gt; 9094:31626/TCP 19h kafka-2-external NodePort 10.100.211.212 &lt;none&gt; 9094:32458/TCP 19h 还可以通过统一的NodePort-svc 将headless暴露至外部 参考：将 headless service 映射到外网 - 简书 (jianshu.com) ❯ kubectl expose svc kafka-headless --name=kafka-external --type=LoadBalancer --port=9094 --target-port=9094 -n database ❯ kubectl get svc -n database kafka-external NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kafka-external LoadBalancer 10.105.60.175 192.168.1.244 9094:32002/TCP 19h 这样也可以使用192.168.1.3:32002 搭建kafka-ui 文档：provectus/kafka-ui: Open-Source Web UI for Apache Kafka Management (github.com) 先修改一下配置文件，k8s内部的kafka 可以直接使用headless服务地址 kafka-ui-config.yaml: apiVersion: v1 kind: ConfigMap metadata: name: kafka-ui-config namespace: database data: config.yml: |- kafka: clusters: - name: k8s bootstrapServers: kafka-headless.database.svc.cluster.local:9092 auth: type: disabled management: health: ldap: enabled: false 使用k8s搭建 kubectl apply -f kafka-ui-config.yaml helm repo add kafka-ui https://provectus.github.io/kafka-ui helm install my-kafka-ui -n database --set yamlApplicationConfigConfigMap.name=&quot;kafka-ui-config&quot;,yamlApplicationConfigConfigMap.keyName=&quot;config.yml&quot; kafka-ui/kafka-ui --version 0.4.5 ","link":"https://blog.marun.run/k8s-an-zhuang-kafka-kafka-ui/"},{"title":"Istio 服务网格启动顺序","content":"记录一下istio服务网格的问题 前言 如果启用了istio 服务网格，那么会自动的注入 istio-proxy sidecar 这里的proxy容器与我们的业务容器启动顺序是不确定的，如果在istio-proxy未正确启动之前，在业务容器中就尝试对外进行网络通信，这个时候就会出现问题。 例如：数据库连接 redis连接初始化问题 所以需要注意在业务代码中做好重试，当然现在大多数框架都自带重连机制。 但在实际业务场景下，一旦开启了日志告警，每次服务重启就会导致一堆告警，会很麻烦，这个时候就还是需要控制一下istio 服务网格容器的启动。 解决方法 高版本使用istio自带配置 自Istio 1.7 版本开始，增加了holdApplicationUntilProxyStarts配置项，解决上述问题 参考官网的changelog 低版本使用手动控制 istio-proxy 有一个健康检查的接口，我们可以通过主动探测的方式来确认istio-proxy启动完成，最后再启动自己的服务 while [[ &quot;$(curl -s -o /dev/null -w ''%{http_code}'' localhost:15020/healthz/ready)&quot; != '200' ]]; do echo Waiting for Sidecar;sleep 1; done; echo Sidecar available; ./http 注意以上脚本 最后的./http 请替换成自己的业务运行脚本 如果运行参数过多，可以自行编写start.sh 启动脚本，或者使用supervision 参考： https://zhuanlan.zhihu.com/p/369301902 ","link":"https://blog.marun.run/istio-fu-wu-wang-ge-qi-dong-shun-xu/"},{"title":"kotlin jackson  missing type id property '@class'","content":"使用kotlin 搭配redis 存储json格式数据的问题 Redis 配置如下 @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate&lt;String, Object&gt; redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(redisConnectionFactory); // json 序列化 redisTemplate.setValueSerializer(RedisSerializer.json()); RedisSerializer&lt;String&gt; stringRedisSerializer = RedisSerializer.string(); // 使用StringRedisSerializer来序列化和反序列化redis的key值 redisTemplate.setKeySerializer(stringRedisSerializer); // hash的key也采用String的序列化方式 redisTemplate.setHashKeySerializer(stringRedisSerializer); // hash的value序列化方式采用jackson redisTemplate.setHashValueSerializer(RedisSerializer.json()); return redisTemplate; } 在使用redistemplate 时，我们使用json格式默认存储value，然后发现每次存进去的数据都会少@class 在取出数据反序列化的时候会报错 GenericJackson2JsonRedisSerializer 文件 public GenericJackson2JsonRedisSerializer(@Nullable String classPropertyTypeName) { this(new ObjectMapper()); registerNullValueSerializer(this.mapper, classPropertyTypeName); if (StringUtils.hasText(classPropertyTypeName)) { this.mapper.enableDefaultTypingAsProperty(DefaultTyping.NON_FINAL, classPropertyTypeName); } else { this.mapper.enableDefaultTyping(DefaultTyping.NON_FINAL, As.PROPERTY); } } 查看源码，发现在这个jackson的配置中，默认使用了DefaultTyping.NON_FINAL 而在 kotlin中 我们的类 默认是final的 需要添加在类前面添加open修饰符 或者你也可以使用使用自定义objectMapper 并设置 ObjectMapper.DefaultTyping.EVERYTHING ","link":"https://blog.marun.run/kotlin-jackson-missing-type-id-property-class/"},{"title":"Mybastis Plus 使用mapper.xml","content":"记录一下mybatis plus 中mapper的使用 mybatis plus官网 分页器配置 @Configuration public class MybatisPlusConfig { @Bean public MybatisPlusInterceptor mybatisPlusInterceptor() { MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor(); interceptor.addInnerInterceptor(new PaginationInnerInterceptor(DbType.MYSQL)); return interceptor; } } mapper.xml: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;mapper.HiProcinstMapper&quot;&gt; &lt;select id=&quot;getTodoList&quot; resultType=&quot;model.HiProcinstModel&quot;&gt; select PROC.* from ACT_HI_PROCINST PROC left join ACT_RU_TASK TASK on PROC.PROC_INST_ID_ = TASK.PROC_INST_ID_ ${ew.customSqlSegment} &lt;/select&gt; &lt;!-- --&gt; &lt;select id=&quot;getDoneList&quot; resultType=&quot;model.HiProcinstModel&quot;&gt; select PROC.* from ACT_HI_PROCINST PROC left join ACT_HI_TASKINST TASK on PROC.PROC_INST_ID_ = TASK.PROC_INST_ID_ ${ew.customSqlSegment} &lt;/select&gt; &lt;/mapper&gt; mapper.kt package mapper; import com.baomidou.mybatisplus.core.conditions.Wrapper import model.HiProcinstModel; import com.baomidou.mybatisplus.core.mapper.BaseMapper; import com.baomidou.mybatisplus.core.metadata.IPage import com.baomidou.mybatisplus.core.toolkit.Constants import org.apache.ibatis.annotations.Param import org.apache.ibatis.annotations.Select interface HiProcinstMapper : BaseMapper&lt;HiProcinstModel&gt; { /** * 获取代办列表 */ fun getTodoList( page:IPage&lt;HiProcinstModel&gt;,@Param(Constants.WRAPPER) wrapper: Wrapper&lt;HiProcinstModel&gt;) : IPage&lt;HiProcinstModel&gt; /** * 获取已处理列表 */ fun getDoneList( page:IPage&lt;HiProcinstModel&gt;,@Param(Constants.WRAPPER) wrapper: Wrapper&lt;HiProcinstModel&gt;) : IPage&lt;HiProcinstModel&gt; } 我们只在mapper.xml中写了关联查询的语句，其他查询条件和分页我们还是使用了mybatisPlus的分页插件 和QueryWrapper 在Service中调用 @Service open class HiProcinstServiceImpl : ServiceImpl&lt;HiProcinstMapper, HiProcinstModel&gt;(), IHiProcinstService { @Resource lateinit var processEngine: ProcessEngine /** * 待审批列表 */ override fun getTodoList(userId: String, pageParams: PageParams): Pagination&lt;ProcInstVo&gt; { val queryWrapper = QueryWrapper&lt;HiProcinstModel&gt;() // 任务分配给当前用户 .eq(&quot;TASK.${HiTaskinstModel.ASSIGNEE_}&quot;, userId) // 并且是活动中的任务 .eq(&quot;PROC.${HiProcinstModel.STATE_}&quot;, HistoricProcessInstance.STATE_ACTIVE) .orderByAsc(&quot;PROC.${HiProcinstModel.START_TIME_}&quot;) val todoList = this.baseMapper.getTodoList(Page(pageParams.page, pageParams.pageSize), queryWrapper) return Pagination(this.convert2Vo(todoList)) } /** * 已处理流程实例 */ override fun getDoneList(userId: String, pageParams: PageParams): Pagination&lt;ProcInstVo&gt; { val queryWrapper = QueryWrapper&lt;HiProcinstModel&gt;() // 任务分配给当前用户 .eq(&quot;TASK.${HiTaskinstModel.ASSIGNEE_}&quot;, userId) // 并且任务已完成 .isNotNull(&quot;TASK.${HiTaskinstModel.END_TIME_}&quot;) .orderByDesc(&quot;PROC.${HiProcinstModel.START_TIME_}&quot;) val doneList = this.baseMapper.getDoneList(Page(pageParams.page, pageParams.pageSize), queryWrapper) return Pagination(this.convert2Vo(doneList)) } } ","link":"https://blog.marun.run/mybastis-plus-shi-yong-mapperxml/"},{"title":"Springboot 注册ServletContextListener  与context-params使用","content":"背景说明： 在springboot中使用 camunda-rest 会有配置日期格式的需求 而按照文档来配置 需要配置在WEB-INF/web.xml里，在springboot中我们并不需要配置这个web.xml 注册camunda的Listenter到springboot中： @Configuration public class MyConfiguration { /** * 把自定义的listener注册到容器中 * @return */ @Bean public ServletListenerRegistrationBean customJacksonDateFormatListener(){ ServletListenerRegistrationBean&lt;CustomJacksonDateFormatListener&gt; registrationBean = new ServletListenerRegistrationBean&lt;&gt;(new CustomJacksonDateFormatListener()); return registrationBean; } } 在application.yaml中配置context-param server: servlet: context-parameters: org.camunda.bpm.engine.rest.jackson.dateFormat: yyyy-MM-dd HH:mm:ss ","link":"https://blog.marun.run/springboot-zhu-ce-servletcontextlistener-yu-context-params-shi-yong/"},{"title":"Mac m1 安装swoole  碰到的问题","content":"使用pecl install swoole 报错 一.openssl not found 使用brew install openssl --enable-openssl --with-openssl-dir=/opt/homebrew/Cellar/openssl@3/3.0.1/ 二. library not found for -latomic 使用 brew install gcc 切换gcc为brew安装的版本 export CC=gcc-11 export CXX=g++-11 三. pcre2.h not found 使用brew install pcre2 然后软链到你当前使用的php版本下的pcre ln -s /opt/homebrew/Cellar/pcre2/10.39/include/pcre2.h /opt/homebrew/Cellar/php@7.3/7.3.31_1/include/php/ext/pcre/pcre2.h 以上路径需要改成您自己的路径 ","link":"https://blog.marun.run/mac-m1-an-zhuang-swoole-peng-dao-de-wen-ti/"},{"title":"activiti, camunda, 会签 或签","content":"会签，或签，依次审批借助于multi-instance Multi-Instance | Camunda Cloud Docs Activiti User Guide 多实例任务： 可以理解为可重复执行的任务 如何定义重复执行的次数 ？ 一. loopCardinality 可以设置一个固定的循环上限。 &lt;multiInstanceLoopCharacteristics isSequential=&quot;true&quot;&gt; &lt;loopCardinality&gt;5&lt;/loopCardinality&gt; &lt;/multiInstanceLoopCharacteristics&gt; 当然你也可以在这里写一个表达式，用于计算出一个正整数循环次数 &lt;multiInstanceLoopCharacteristics isSequential=&quot;true&quot;&gt; &lt;loopCardinality&gt;${nrOfOrders-nrOfCancellations}&lt;/loopCardinality&gt; &lt;/multiInstanceLoopCharacteristics&gt; 二. Collection 设置一个循环访问集合 设置固定的可循环访问的集合：collection=[&quot;marin&quot;, &quot;monkey&quot;] &lt;userTask id=&quot;miTasks&quot; name=&quot;My Task&quot; activiti:assignee=&quot;${assignee}&quot;&gt; &lt;multiInstanceLoopCharacteristics isSequential=&quot;false&quot; activiti:collection=&quot;[&quot;marin&quot;,&quot;monkey&quot;]&quot; activiti:elementVariable=&quot;assigner&quot; &gt; &lt;/multiInstanceLoopCharacteristics&gt; &lt;/userTask&gt; 并通过elementVariable将循环取出的值赋值给一个变量。 在上面的这个例子中，我们设置一个固定的集合[&quot;marin&quot;,&quot;monkey&quot;],将elementVariable 设置为assigner。并将userTask的assignee设置为${assigner} 。 这也就是代表着，在这个多例任务中，会创建两个userTask,并依次将这两个任务分配给marin,monkey。 使用变量填充 collection。 &lt;userTask id=&quot;miTasks&quot; name=&quot;My Task&quot; activiti:assignee=&quot;${assigner}&quot;&gt; &lt;multiInstanceLoopCharacteristics isSequential=&quot;true&quot; activiti:collection=&quot;assignerList&quot; activiti:elementVariable=&quot;assigner&quot; &gt; &lt;/multiInstanceLoopCharacteristics&gt; &lt;/userTask&gt; 我们可以在发起实例的时候传入一个变量assignerList RuntimeService runtimeService = processEngine.getRuntimeService(); Map&lt;String, Object&gt; variables = new HashMap&lt;&gt;(); variables.put(&quot;assignerList&quot;, Arrays.asList(&quot;marin&quot;, &quot;monkey&quot;, &quot;roy&quot;)); runtimeService.startProcessInstanceByKey(&quot;multiple&quot;, variables); 使用spring，服务 &lt;userTask id=&quot;miTasks&quot; name=&quot;My Task&quot; activiti:assignee=&quot;${assigner}&quot;&gt; &lt;multiInstanceLoopCharacteristics isSequential=&quot;false&quot; activiti:collection=&quot;${userService.getApprovalAssigners()}&quot; activiti:elementVariable=&quot;assigner&quot; &gt; &lt;/multiInstanceLoopCharacteristics&gt; &lt;/userTask&gt; 如果我们使用spring boot，我们可以在这里调用代码中的某个服务某个方法来获取一个集合 注意⚠️，这里的userService 必须在spring 中注册为 Spring Bean Service 注意⚠️，一个多实例活动必须要有loopCardinality，或者collection 其中一个，否则执行出错。 如何终止运行？ 默认在所有多实例任务都完成之后，那么这个多实例任务也就完成了。 或者你可以定一个完成条件：completionCondition &lt;userTask id=&quot;miTasks&quot; name=&quot;My Task&quot; activiti:assignee=&quot;${assigner}&quot;&gt; &lt;multiInstanceLoopCharacteristics isSequential=&quot;false&quot; activiti:collection=&quot;${userService.getApprovalAssigners()}&quot; activiti:elementVariable=&quot;assigner&quot; &gt; &lt;completionCondition&gt;${nrOfCompletedInstances/nrOfInstances &gt;= 0.6 }&lt;/completionCondition&gt; &lt;/multiInstanceLoopCharacteristics&gt; &lt;/userTask&gt; 以下为默认的多实例变量。 nrOfInstances: 实例总数（任务总数），对应collection的size，loopCardinality的值 nrOfActiveInstances: 当前活动的实例数，对于串行的多实例来说，这个值一直都是1 nrOfCompletedInstances: 已完成实例数 上面的例子代表，如果有60%的任务已经完成，那么当前多实例就完成了。 当然，这里也可以写表达式，或者调用spring 的service，只要返回的是一个bool值就ok 并签： 并签对应 parallel multi-instance 并行的多实例，如下图（图片来自canmunda） 例如：这一次多实例审批，指定了两个人。 会同时生成两个task，这两个task是相互独立的，只有当这两个独立的task都完成时才会走到下一步。 当然我们可以在多实例任务上定义终止条件，提前结束多例任务。 后文的或签就是基于此实现。 并行的多实例是在流程运行至此的时候，一次生成多个任务。 依次审批： 依次审批对应 sequential multi-instance 串行的多实例 首先生成第一个任务，只有第一个任务完成后才会生成第二个任务，直到所有的任务完成 或签： 或签对应的也是parallel multi-instance, 不同的是，我们会设置一个completionCondition=&quot;${nrOfCompletedInstances==1}&quot; 也就是我们会一次生成多个任务，只要有其中一个任务完成，那么整个多实例任务都被视为完成了。 ","link":"https://blog.marun.run/activiti-camunda-hui-qian-huo-qian/"},{"title":"mysql view 字符集排序报错","content":"General error: 1267 Illegal mix of collations 在使用mysql视图时，报错 SQLSTATE[HY000]: General error: 1267 Illegal mix of collations (utf8mb4_general_ci,COERCIBLE) and (utf8mb4_unicode_ci,COERCIBLE) for operation '=' 通过命令 SHOW CREATE TABLE view_table_name; 查看字符集，以及排序规则 character_set_client collation_connection utf8mb4 utf8mb4_general_ci 发现视图的排序规则错误。 删除原有view 并重建 -- 设置排序规则 SET collation_connection = utf8mb4_unicode_ci; -- 删除原有的view drop VIEW view_table_name; -- 重新新建view create view view_table_name .... 如果是字符集报错，同理修改即可。 SET character_set_client = utf8mb4; SET character_set_results = utf8mb4; SET character_set_connection = utf8mb4; 参考资料： https://stackoverflow.com/questions/9422189/why-is-my-view-utf8-and-how-can-i-change-it-to-latin1 ","link":"https://blog.marun.run/mysql-view-zi-fu-ji-pai-xu-bao-cuo/"},{"title":"部署 packagist私服 搭配gitlab 私服使用 ","content":"部署packagist 搭配gitlab使用 源码地址：composer/packagist 笔者在写这篇文章的时候对应的commit sha :aa0d63fd6ebbfd991aa3ab652d33c6c8e463543a 因为packagist没有版本的概念 所以使用commit sha 的方式来记录一下 环境安装 由于最新版使用的是php8.0,所以来安装一下8.0 使用apt安装php8.0 sudo apt install php8.0 php8.0-bcmath php8.0-redis php8.0-cli php8.0-zip php8.0-mysql php8.0-http php8.0-curl php8.0-common php8.0-raphf php8.0-dev php8.0-mbstring php8.0-dom php8.0-xml 安装过程比较简单，基本如上 在安装完成后，composer install 时出现unrecognised compile-time option bit(s) 按照这个issure 只用执行以下命令即可（如果你没有出现任何问题，无视即可） sudo apt-get install --only-upgrade libpcre2-16-0 libpcre2-32-0 libpcre2-8-0 libpcre2-dev libpcre2-posix2 项目部署 git clone https://github.com/composer/packagist.git &amp;&amp; cd packagist 项目配置 拉取代码之后，先配置一下redis,database 等等 打开.env文件 # 数据库配置， 修改成自己的配置，并确保拥有packagist库 DATABASE_URL=&quot;mysql://root:123456@127.0.0.1:3306/packagist?serverVersion=8.0&quot; # ALGOLIA 配置,搜索用。 可前往 https://www.algolia.com/ 注册，免费使用，如果只是尝试本地部署，可忽略 ALGOLIA_APP_ID=xxx ALGOLIA_ADMIN_KEY=xxx ALGOLIA_SEARCH_KEY=xxx ALGOLIA_INDEX_NAME=xxx # redis 配置，可换成自己 REDIS_URL=redis://localhost # 谷歌的一个验证，我在使用时，这里会有点问题，下面会讲。 ###&gt; beelab/recaptcha2-bundle ### APP_RECAPTCHA_ENABLED=false APP_RECAPTCHA_SITE_KEY=needed APP_RECAPTCHA_SECRET=needed ###&lt; beelab/recaptcha2-bundle ### apcu缓存错误 config/packages/cache.yaml 文件里的system,使用的apcu,由于我的php没有装这个扩展导致缓存报错 framework: cache: app: cache.adapter.redis # 下面的这个system缓存 使用的是apcu。如果你没有安装apcu这个php扩展，可以替换成其他的 # 例如：cache.adapter.redis cache.adapter.filesystem 等等 system: cache.adapter.apcu default_redis_provider: snc_redis.cache pools: doctrine.cache: null prefix_seed: packagist recaptcha2的问题 查阅以下代码。 templates/registration/register.html.twig templates/reset_password/request.html.twig 我们会发现这个设置了一个set requiresRecaptcha = true，这个会让我们上面.env中所设置的APP_RECAPTCHA_ENABLED=false 无效。 这样就会导致如果你没有设置APP_RECAPTCHA_SITE_KEY和APP_RECAPTCHA_SECRET，那么你就将永远无法提交注册表单，和重置密码的表单。 这个问题困扰了我很久，具体表现为，点击注册按钮，没有任何反应，只有console控制台报了一下recaptcha.js的错误。 如果你跟我一样，并没有设置recaptcha,那么你需要删除上面两个代码文件中的{% set requiresRecaptcha = true %} 并且再次重新 composer install，或者手动运行php bin/console cache:clear 当你都已经准备好了之后，就可以使用composer install来安装你的程序了。 然后使用symfony serve 启动一个简单的webserver 如果你没有symfony,请到这里下载一个 https://symfony.com/download 提交自己的包 一旦你成功完成以上操作，成功注册了一个账号之后，你可以尝试提交自己的私有包。 例如 https://github.com/xxx/xxx，https://gitlab.com/xxx/xxx 但，需要记住的是： 这个链接对应的代码库应该是一个 pulic 公开的库,否则你需要到你部署packagist项目的机器上设置ssh 或者 设置 git clone https时的账号密码 而且你的代码库中，应该拥有composer.json文件，并且正确的配置composer.json内容。 一旦提交成功，你可以点击update按钮更新你的私有包，记住要在服务器上运行php bin/console packagist:run-workers 当然 最后的做法是按照文档上的，在服务器上使用crontab 运行上面的命令。 使用自己的包 一旦上面的都完成了，那么你就已经成功的提交了一个包到自己私有的packagist上了 在另一个项目中的composer.json中加上 自己的仓库 &quot;repositories&quot;: [ { &quot;type&quot; : &quot;composer&quot;, &quot;url&quot; : &quot;https://packagist.mydomain.com&quot; } ] 然后composer require my/packagist，你就可以成功的下载 配置私有的gitlab-domains 正常来说，我们的包应该拥有一个版本号，并且可以下载dist 但如果我们使用私有的gitlab,每次composer install 或者 composer require 时 是使用 git clone的方式去拉取源码。 你可以使用 composer require xxx/xxx -vvv的方式来查看composer 下载每个包的细节 这样就会对你composer install时的环境有要求，必须拥有git 命令。 在一些k8s,docker 镜像中install 时可能会出现一些些问题。 我们在packagist部署的机器上运行下面的这个命令，再次去packagist中更新自己的包 然后再rm -rf vendor &amp;&amp; rm -rf composer.lock &amp;&amp; composer install -vvv 你就可以看到以dist的方式去gitlab下载源码了，不再是git clone的方式去克隆源码 composer --global config gitlab-domains gitlab.xxx.cn gitlab.com ","link":"https://blog.marun.run/bu-shu-packagist-si-fu-da-pei-gitlab-si-fu-shi-yong/"},{"title":"gitlab-ci/cd","content":"简单介绍gitlab-ci/cd的使用 官方文档： GitLab CI/CD 没有比官方文档更好不过的教程了！ gitlab-ci如何运行？ gitlab-ci并不是平白无故运行起来的，依托于一个叫做GitLab Runner 的东西，我们配置的gitlab-ci 才会顺利的运行。 如果你对Install GitLab Runner 感兴趣，可以参看文档，gitlab-runner是如何安装的。 ci/cd的运行 至少需要一个gitlab-runner。当然也可以配置多个，比如您可以在macos系统上安装gitlab-runner并注册到gitlab中，这样您就可以将类似ios打包的过程放到这个runner上了。以此类推，如果您有特殊要求的ci/cd，可以指定特殊的runner。 ci/cd运行过程： gitlab按照您项目中的的gitlab-ci.yaml配置，创建一个或多个job,然后等待gitlab-runner运行这些job。 大概的过程类似于： 选择一个执行器 也就是执行命令等操作的环境，需要gitlab-runner中配置。例如shell也就是在gitlab-runner主机上运行。docker就是拉取某个镜像，在镜像中执行命令。 下文的runner的执行器均为docker 拉取源码 重建缓存 如果当前job或者流水线有配置缓存，会拉取缓存。（下文详细说） 下载artifacts 如果当前job的上游有配置artifacts，会下载artifacts（下文详细说） 执行命令 执行您的scripts CI/CD pipelines 流水线 顾名思义，流水线 ，也就是定义整个流程需要完成哪些阶段，每个阶段需要做什么。 Stages 阶段： 定义了整个pipelines需要做哪些事，这些事需要顺序执行。 例如：先下载依赖，然后编译代码，再推送编译后的代码，最后发一个ci/cd成功的通知 Jobs 作业： 每个阶段需要完成的操作，例如：使用某个php镜像，执行composer install 命令下载依赖。 以下为官方示例为例： stages: # 阶段 - build - test - deploy image: alpine # 作业 build_a: stage: build script: - echo &quot;This job builds something.&quot; build_b: stage: build script: - echo &quot;This job builds something else.&quot; test_a: stage: test script: - echo &quot;This job tests something. It will only run when all jobs in the&quot; - echo &quot;build stage are complete.&quot; test_b: stage: test script: - echo &quot;This job tests something else. It will only run when all jobs in the&quot; - echo &quot;build stage are complete too. It will start at about the same time as test_a.&quot; deploy_a: stage: deploy script: - echo &quot;This job deploys something. It will only run when all jobs in the&quot; - echo &quot;test stage complete.&quot; deploy_b: stage: deploy script: - echo &quot;This job deploys something else. It will only run when all jobs in the&quot; - echo &quot;test stage complete. It will start at about the same time as deploy_a.&quot; 对于更多内容，可参考官方文档 Jobs 作业 作业才是流水线每个阶段实际执行的内容，每个作业都应该定义在顶级，并且拥有至少一个script。 您可以理解为流水线的工人 以官方文档为例： job1: # 作业的名称 script: &quot;execute-script-for-job1&quot; # 作业执行的脚本 job2: # 作业的名称 script: &quot;execute-script-for-job2&quot; # 作业执行的脚本 作业定义的数量不受限制，每个作业都是独立的单独运行。 ⚠️ 每个作业都是单独，独立运行。所以，即使在同一个流水线中 也就是在一次ci/cd中。每个作业运行的runner都可能是不同的。 例如：你在runner1 上拉取了依赖，你在runner2上编译了代码，你在runner3上发送了一个通知。 那么，如果在deploy时想使用上一个阶段编译出来的文件该如何操作呢？ Caching / Artifacts 缓存和制品 Artifacts 制品，可以理解为作业产出的内容（上个流水线工人产出的内容） 简单以下面这个为例： stages: - install - build - deploy install-online: stage: install image: tool/yarn:v1 script: - yarn install - yarn build:prod # 将yarn build 打包出来的dist 作为制品，传到下一个job artifacts: paths: - dist/ # 默认情况下，后面阶段的作业会自动下载由前面阶段的作业创建的所有制品。 build-online-image: stage: build image: tool/docker:latest only: - master script: - docker build -t $ONLIE_IMAGE_NAME . - docker push $ONLIE_IMAGE_NAME - docker tag $ONLIE_IMAGE_NAME $IMAGE - docker push $IMAGE deploy-online: stage: deploy image: kubematrix/cli:v1 # 如果您不需要依赖任何的制品，你可以使用dependencies: [] dependencies: [] only: - master script: - matrix workloads deploy 在install时安装依赖并打包出一个dist文件夹，使用artifacts将这个文件夹打包出来。 在install执行完成后，build阶段时会自动下载之前的artifacts 也就是dist文件夹，完成了将上个阶段编译出来的文件带到下个阶段的操作。 dependencies 依赖 可参考官方文档， Cache 缓存 以php为例: # # https://gitlab.com/gitlab-org/gitlab/-/tree/master/lib/gitlab/ci/templates/PHP.gitlab-ci.yml # image: php:7.2 # Cache libraries in between jobs cache: key: $CI_COMMIT_REF_SLUG paths: - vendor/ before_script: # Install and run Composer - curl --show-error --silent &quot;https://getcomposer.org/installer&quot; | php - php composer.phar install test: script: - vendor/bin/phpunit --configuration phpunit.xml --coverage-text --colors=never 以$CI_COMMIT_REF_SLUG(预定义变量，这里是指分支的名称)为key， 将vendor/目录缓存下来。 缓存下来 以后的每一次作业，都会下载缓存，这样会加速composer install 的速度 ⚠️需要注意的点： cache:key 如果不指定key，那么默认同一个项目的所有build共享这个缓存。关于cache:key的各种场景使用，可参考：Caching in GitLab CI/CD | GitLab 默认cache使用本地磁盘保存，但这样会出现一个问题。如果每次job运行都在不同的runner上，那么缓存是不可以共享的。这时候就需要引入一个分布式存储这个缓存文件。 配置阿里云OSS存储cache文件 Advanced configuration | GitLab 为了统一缓存，我们需要使用OSS 首先进入gitlab-runner对应的主机，gitlab-runner list 查看配置文件存放的位置 /etc/gitlab-runner/config.toml [[runners]] name = &quot;cloud5&quot; url = &quot;https://gitlab.xxx.cn/&quot; token = &quot;token&quot; executor = &quot;docker&quot; [runners.custom_build_dir] [runners.cache] Type = &quot;s3&quot; Shared = true Path = &quot;xxx&quot; [runners.cache.s3] ServerAddress = &quot;oss-cn-hangzhou.aliyuncs.com&quot; AccessKey = &quot;key&quot; SecretKey = &quot;sec&quot; BucketName = &quot;gitlab-runner-cached&quot; BucketLocation = &quot;oss-cn-hangzhou&quot; [runners.docker] tls_verify = false image = &quot;ruby:2.6&quot; privileged = false disable_entrypoint_overwrite = false oom_kill_disable = false disable_cache = false volumes = [&quot;/var/run/docker.sock:/var/run/docker.sock&quot;, &quot;/root/.docker/:/root/.docker/&quot;, &quot;/data/gitlab-runner/cache:/cache&quot;, &quot;/data/gitlab-runner/npm-cache:/root/.npm&quot;, &quot;/data/gitlab-runner/composer-cache:/root/.composer&quot;] shm_size = 0 pull_policy = &quot;if-not-present&quot; 主要内容为以下： [runners.cache] Type = &quot;s3&quot; # 缓存类型 Shared = true # 缓存在多个runner之间共享 Path = &quot;xxx&quot; # 缓存路径前缀，如果一个gitlab-runner在多个gitlab中注册，可以使用前缀的方式做区分 [runners.cache.s3] ServerAddress = &quot;oss-cn-hangzhou.aliyuncs.com&quot; # oss地址 AccessKey = &quot;key&quot; # key SecretKey = &quot;sec&quot; # secret BucketName = &quot;gitlab-runner-cached&quot; # bucketName BucketLocation = &quot;oss-cn-hangzhou&quot; # BucketLocation 以此类推，你需要在所有的gitlab-runner中配置此缓存，这样所有的缓存都会被上传到阿里云的OSS中，无论你的job运行在哪个runner中。 示例： 以一个前端项目为例： # 定义 variables: ONLIE_IMAGE_NAME: registry.cn-hangzhou.aliyuncs.com/$CI_PROJECT_NAME:$CI_COMMIT_SHORT_SHA stages: - preInstall - install - build - deploy build-image: stage: build image: docker only: - test script: - docker build -t $IMAGE . - docker push $IMAGE # ---------------------------------- master ---------------------------------- # 只有当package.json变化时才会重新install preInstall-job: stage: preInstall image: yarn:v1 cache: key: $CI_COMMIT_REF_SLUG paths: - node_modules/ - yarn.lock # 只需要推送缓存，不需要拉取缓存 policy: push only: refs: - master changes: - package.json script: - yarn install install-online: stage: install image: yarn:v1 cache: key: $CI_COMMIT_REF_SLUG paths: - node_modules/ - yarn.lock # 只需要拉取缓存，不需要推送缓存 policy: pull only: - master script: # 当缓存存在时不需要install - if [ ! -d &quot;./node_modules/&quot; ];then yarn install; else ls -alh; fi - yarn build:prod # 打包制品给后面的job artifacts: paths: - dist/ build-online-image: stage: build # 使用docker镜像 image: docker:latest # 只在master分支变动时运行这个job only: - master script: - docker build -t $ONLIE_IMAGE_NAME . # 构建镜像 - docker push $ONLIE_IMAGE_NAME # 推送镜像 deploy-online: stage: deploy # 使用kubematrix-cli镜像 image: kubematrix/cli:v1 # 依赖为空，表示不需要任何的依赖 dependencies: [] only: - master script: - matrix workloads deploy 示例讲解 GitLab CI/CD variables | GitLab 变量 可以在文档Predefined variables reference | GitLab 中查看预定义的变量 首先定义变量 ONLIE_IMAGE_NAME 以供使用 定义stages 流水线阶段 定义了4个阶段，preInstall，install，build，deploy，下面的job将按照关联的stag顺序执行 定义job stag 定义了job对应的阶段。 image：定义了执行脚本所使用的docker镜像。 only ： 定义了job只在某些情况下执行。默认为only.refs输入分支名，或与分支相匹配的正则表达式。only.changes 检测只在某些文件变动时运行。 cache ： 定义了改job使用缓存，cache.policy 缓存规则，默认为pull-push开始job前拉取缓存,job成功后推送缓存。 可修改为push只推送，不拉取。pull只拉取，不推送 artifacts： 定义了需要打包的制品，给后面的job使用 dependencies : 需要的依赖，如果上面的job有artifacts 会默认下载，如果不需要artifacts 可将这个属性定义为空数组 ","link":"https://blog.marun.run/gitlab-cicd/"},{"title":"android逆向 360加固apk，frida Hook java代码","content":"参考文章： https://www.52pojie.cn/forum.php?mod=viewthread&amp;tid=1287307 书接上文，这次我会尝试apk的逆向，找到app不能抓包的原因 如图所示，这个app被360加固了，几经百度，按照开头那篇文章的参考，我们得到了dex文件 将多个dex文件 移动到电脑上 由于有多个dex文件，我们需要合并查看代码 jadx工具： https://github.com/skylot/jadx/ 具体代码如下 import os, sys # python3.7 merge_dex.py ./file/ livedex if __name__ == &quot;__main__&quot;: if len(sys.argv) &lt; 3 : print(&quot;start error&quot;) sys.exit() print(sys.argv[1], sys.argv[2]) path = sys.argv[1] #文件夹目录 files= os.listdir(path) #得到文件夹下的所有文件名称 s = [] for file in files: #遍历文件夹 if file.find(&quot;dex&quot;) &gt; 0: ## 查找dex 文件 sh = 'jadx.bat -j 1 -r -d ' + sys.argv[2] + &quot; &quot; + path + file print(sh) os.system(sh) 最终结果就是这样了，具体代码就不放了。 经过查看源码，发现是okHttp设置了no_proxy 只需要将这一部分代码hook掉就ok了。 我使用的frida ：https://github.com/frida/frida 电脑使用pip install // 电脑安装frida pip install frida-tools # CLI tools pip install frida # Python bindings // 将frida-server推到手机 adb.exe push .\\frida-server-14.0.7-android-arm64 /data/local/tmp/frida-server // 连接到手机 adb shell cd /data/local/tmp // 切到root用户 su // 修改权限 chmod 777 frida-server // 执行 ./frida-server 上面就将手机的frida-server开启了，然后在电脑上试试是否可以成功连接 frida-ps.exe -U 如果有显示手机相关进程就ok 将自己编写的js代码推到手机 进行 hook frida -U --no-pause -f com.showstartfans.activity -l .\\showstart.js showstart.js 代码: Java.perform( function () { var application = Java.use(&quot;android.app.Application&quot;); application.attach.overload('android.content.Context').implementation = function(context) { var result = this.attach(context); // 先执行原来的attach方法 var classloader = context.getClassLoader(); Java.classFactory.loader = classloader; var Hook_class = Java.classFactory.use(&quot;com.taihebase.activity.utils.SecurityUtil&quot;); console.log(&quot;Hook_class: &quot; + Hook_class); Hook_class.getNeedCapturePacket.implementation = function(){ return true; } return result; } } ) frida关于java的官方文档: https://frida.re/docs/javascript-api/#java 这个时候就可以使用fiddler安心的抓接口了 故技重施~ 又可以流畅的下单购票了。 ","link":"https://blog.marun.run/android-ni-xiang-360-jia-gu-apkfrida-hook-java-dai-ma/"},{"title":"fiddler-实战 ","content":"前言： 前段时间看了一档B站的综艺：《说唱新世代》，最近节目里面的选手开始巡演。想买票去看看的，在秀动上没买到票，很气。so，想想办法吧，于是有了以下记录。 一，尝试APP抓包看看 抓包工具：Fiddler 具体配置不再赘述，网上有很多文章都有讲 App配置wifi代理8888 很可惜，App并没有成功，只抓到几张图片的😭 二，尝试wap端 既然App端不可以的话，那就尝试一下wap端吧。 呐，这里显示只能去app购买，可恶😤，右键检查一下。 这里加入了一个id，扒下js，detail.js 这里判断了activity.type == 6 ，看下接口，哪里有type : 6的 呐，不出意外就是这里了，我们使用fillder 拦截response ，修改这个type 第一步，双击这玩意儿，开启response拦截功能 （注意这里会拦截所有的response，在没有抓到你想要的接口时，先点上面的go 放行所有的请求，直到你想要的接口出现） 第二步，搜到想要修改的值，我这里就是&quot;type&quot;:6 改为type:1 第三步，修改成功后点击这个run to xxxx的绿色按钮，再把第一步这里还原就行 好了，来看看现在的页面 从只能app购买，变成了立即购票 nice，兄弟。第一步完成了 我们点击立即购票 可惜这里显示即将开售，我们来看看这个地方的接口 这里我们故技重施，把这里的0 都改成10， 注意这里的salesStatus也需要改成1 呐，这里就可以直接点立即购买了,点进去就下单页了。 这里的已售罄其实也可以改，同样通过fiddler 改接口，我就不再演示了。 by the way , 这里如果直接进来的话，其实直接通过url下次再进来了，就不用前面的几步操作了，链接放下面 https://wap.showstart.com/pages/order/activity/confirm/confirm?sequence=123744&amp;ticketId=2d80ca0d806844cba6915518356f76c6&amp;ticketNum=1 ","link":"https://blog.marun.run/fiddler-shi-zhan-yi/"},{"title":"mysql中 group by排序，derived_merge优化的坑","content":"一个简单的表 CREATE TABLE `test` ( `id` int(11) NOT NULL AUTO_INCREMENT, `spu_id` int(11) DEFAULT NULL, `price` decimal(10,2) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci; 大概内容 id spu_id price 1 100 200 2 100 100 3 200 400 4 200 200 对spu_id进行分组，按price从小到大排序： SELECT * FROM `test` GROUP BY spu_id ORDER BY price 直接使用group by 查出来的数据是按id顺序分组的，并未达到预期 尝试使用子查询，先排序再分组 SELECT * FROM ( SELECT * FROM `test` ORDER BY price ) AS tmp GROUP BY spu_id 注意：这个方式在低版本中有效。在5.7版本中引入新特性 derived_merge优化过后无效了。 具体无效原因我们可使用explain 分析 EXPLAIN SELECT * FROM ( SELECT * FROM `test` ORDER BY price) AS tmp GROUP BY spu_id; 如图所示： MySQL 将子查询优化成了一个简单查询，子查询中的排序无效~ 解决方法： 将derived_merge 关闭 SET optimizer_switch='derived_merge=off'; SET GLOBAL optimizer_switch='derived_merge=off'; 使用特殊的查询阻止 derived_merge 优化 可以通过在子查询中使用任何阻止合并的构造来禁用合并，尽管这些构造对实现的影响不那么明显。防止合并的构造与派生表和视图引用相同： 聚合函数（SUM()， MIN()， MAX()， COUNT()，等等） DISTINCT GROUP BY HAVING LIMIT UNION 要么 UNION ALL 选择列表中的子查询 分配给用户变量 仅引用文字值（在这种情况下，没有基础表） 以上内容参考文档：mysql文档 那么我们可以将上面的那条sql语句修改为： SELECT * FROM ( SELECT * FROM `test` HAVING 1=1 ORDER BY price ) AS tmp GROUP BY spu_id; 使用 having 来阻止合并 那么再用explain看看 如有错误请指正~ 请多包涵 ","link":"https://blog.marun.run/mysql-group-by-pai-xu-derived_merge-you-hua-de-keng/"},{"title":" 在 Hyperf 框架中使用 prometheus + grafana 部署基本的监控","content":"参考： hyperf利用prometheus接入服务监控,使用grafana实现数据的实时监控显示 hyperf文档 本文章记录本人的第一次部署所踩的坑，未深入了解prometheus 和grafana 如有不当的地方请指正，谢谢！ 一. 使用docker-compose部署 version: '2' networks: monitor: driver: bridge services: prometheus: image: prom/prometheus container_name: prometheus hostname: prometheus restart: always volumes: # 将你的prometheus.yml文件放在当前文件同级下，或自定义 - ./prometheus.yml:/etc/prometheus/prometheus.yml #- /home/prometheus/node_down.yml:/etc/prometheus/node_down.yml ports: - &quot;9090:9090&quot; networks: monitor: ipv4_address: 172.18.0.3 grafana: image: grafana/grafana container_name: grafana hostname: grafana restart: always volumes: # 创建 etc目录，data目录存储grafana的数据 - ./etc:/etc/grafana - ./data:/var/lib/grafana ports: - &quot;3000:3000&quot; networks: monitor: ipv4_address: 172.18.0.4 node-exporter: image: prom/node-exporter container_name: node-exporter hostname: node-exporter restart: always ports: - &quot;9100:9100&quot; networks: monitor: ipv4_address: 172.18.0.2 注意：为了避免每次docker-compose 启动之后 ip会发生变化，我这里配置了固定IP，请根据个人实际情况配置，或参阅docker相关文档 使用命令docker-compose up启动容器 二. 项目配置 因为对 prometheus的不了解，我直接使用hyperf默认配置 引入组件 composer require hyperf/metric 发布默认配置文件 php bin/hyperf.php vendor:publish hyperf/metric 在config/autoload/dependencies.php中添加对应的Redis存储 return [ \\Prometheus\\Storage\\Adapter::class =&gt; \\Hyperf\\Metric\\Adapter\\Prometheus\\RedisStorageFactory::class, ]; 在上面的第一篇文章中，老哥说使用swoole_table更高效，我还不知道如何使用，有兴趣的老哥可以自己研究一下。 增加中间件 在config/autoload/middlewares.php文件中增加对应的中间件 return [ 'http' =&gt; [ \\Hyperf\\Metric\\Middleware\\MetricMiddleware::class, ], ]; 添加 metrics路由 Router::get('/metrics', function(){ $registry = Hyperf\\Utils\\ApplicationContext::getContainer()-&gt;get(Prometheus\\CollectorRegistry::class); $renderer = new Prometheus\\RenderTextFormat(); return $renderer-&gt;render($registry-&gt;getMetricFamilySamples()); }); 这样对项目的配置就完成了 三. prometheus的配置 在 prometheus.yml文件中增加对应的配置 scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: ['localhost:9090'] - job_name: 'node' # 注意这里的IP需要填写 node-exporter 容器的ip static_configs: - targets: ['172.18.0.2:9100'] - job_name: 'skeleton' # 这里填写的是宿主机的ip static_configs: - targets: ['10.0.75.1:9502'] 配置完成之后，再次 dokcer-compose up 访问 http://localhost:9090 查看 prometheus 如图所示，node 和 skeleton 都已启动 四. Grafana 配置 上面都配置完了，开始配置 Grafana 打开 http://localhost:3000 默认密码是: admin/admin 新建datasource 左侧边栏 add datasources 选择Prometheus 配置 datasource 填写容器的IP:端口 导入hyperf官方的JSON文件 导入之后需要将默认的 app_name改成你自己的 如：admin-api 就需要填写admin_api 改成下划线形式 查看监控 在Home中你就可以看到了 点进去查看 到此结束，小白第一次配置监控，还有很多东西没弄清楚 ","link":"https://blog.marun.run/hyperf-grafana/"}]}